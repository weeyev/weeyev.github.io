[
  {
    "title": "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures",
    "url": "https://arxiv.org/pdf/2402.15332",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "John Carmack on Idea Generation",
    "url": "https://amasad.me/carmack",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "Last year at an internal talk at Facebook I was fortunate to see [John Carmack](https://en.wikipedia.org/wiki/John_Carmack) speak about his idea generation system. At first I was disappointed...",
    "full_snippet": "Last year at an internal talk at Facebook I was fortunate to see [John Carmack](https://en.wikipedia.org/wiki/John_Carmack) speak about his idea generation system. At first I was disappointed because...",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Category: The Essence of Composition | \u00a0\u00a0Bartosz Milewski's Programming Cafe",
    "url": "https://bartoszmilewski.com/2014/11/04/category-the-essence-of-composition",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "I was overwhelmed by the positive response to my previous post, the Preface to Category Theory for Programmers. At the same time, it scared the heck out of me because I realized what high...",
    "full_snippet": "I was overwhelmed by the positive response to my previous post, the Preface to Category Theory for Programmers. At the same time, it scared the heck out of me because I realized what high expectations people were placing in me. I\u2019m afraid that no matter what I\u2019ll write, a lot of readers will be disappointed. Some readers would like the book to be more practical, others more abstract. Some hate C++ and would like all examples in Haskell, others hate Haskell and demand examples in Java. And I know that the pace of exposition will be too slow for some and too fast for others. This will not be the perfect book. It will be a compromise. All I can hope is that I\u2019ll be able to share some of my aha! moments with my readers. Let\u2019s start with the basics. A category is an embarrassingly simple concept. A category consists of objects and arrows that go between them. That\u2019s why categories are so easy to represent pictorially. An object can be drawn as a circle or a point, and an arrow\u2026 is an arrow.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Mixture of Experts Explained",
    "url": "https://huggingface.co/blog/moe",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "With the release of Mixtral 8x7B (announcement, model card), a class of transformer has become the hottest topic in the open AI community: Mixture of Experts, or MoEs for short. In this blog post, we...",
    "full_snippet": "With the release of Mixtral 8x7B (announcement, model card), a class of transformer has become the hottest topic in the open AI community: Mixture of Experts, or MoEs for short. In this blog post, we take a look at the building blocks of MoEs, how they\u2019re trained, and the tradeoffs to consider when serving them for inference. Let\u2019s dive in! MoEs: Let\u2019s dive in! The scale of a model is one of the most important axes for better model quality. Given a fixed computing budget, training a larger model for fewer steps is better than training a smaller model for more steps. Mixture of Experts enable models to be pretrained with far less compute, which means you can dramatically scale up the model or dataset size with the same compute budget as a dense model. In particular, a MoE model should achieve the same quality as its dense counterpart much faster during pretraining. So, what exactly is a MoE? In the context of transformer models, a MoE consists of two main elements: So, to recap, in MoEs",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Futurist Manifesto, by Filippo Tommaso Marinetti",
    "url": "https://www.arthistoryproject.com/artists/filippo-tommaso-marinetti/the-futurist-manifesto",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "We had stayed up all night, my friends and I, under hanging mosque lamps with domes of filigreed brass, domes starred like our spirits, shining like them with the prisoned radiance of electric...",
    "full_snippet": "We had stayed up all night, my friends and I, under hanging mosque lamps with domes of filigreed brass, domes starred like our spirits, shining like them with the prisoned radiance of electric hearts. For hours we had trampled our atavistic ennui into rich oriental rugs, arguing up to the last confines of logic and blackening many reams of paper with our frenzied scribbling. An immense pride was buoying us up, because we felt ourselves alone at that hour, alone, awake, and on our feet, like proud beacons or forward sentries against an army of hostile stars glaring down at us from their celestial encampments. Alone with stokers feeding the hellish fires of great ships, alone with the black spectres who grope in the red-hot bellies of locomotives launched on their crazy courses, alone with drunkards reeling like wounded birds along the city walls. Suddenly we jumped, hearing the mighty noise of the huge double-decker trams that rumbled by outside, ablaze with colored lights, like village",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "returning to stanford | writing",
    "url": "https://masonjwang.com/writing/returning-to-stanford",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "Every reinvention story is a confession, and mine is this: before I started at Stanford this September, when I had often thought I would never return, I was very dismissive of what college had to...",
    "full_snippet": "Every reinvention story is a confession, and mine is this: before I started at Stanford this September, when I had often thought I would never return, I was very dismissive of what college had to offer. I had two gap years in my pockets: living in Rome with other founders; crying after firing someone for the first time; traveling the world, hiking in Uzbekistan, meeting my cofounder's family in Abu Dhabi; sneaking into Stanford dining halls. While I never admitted it, my mindset reeked of how proudly I had weaponized myself. My plan was to do one quarter at Stanford, learn the math that was bottlenecking me, methodically explore, then find a \u201crocketship\u201d to leave for. I finished my first quarter last week. It rushes past the same way: biking through campus feeling like a kid, laughing with new friends harder than I have in my life. I have found a magical place that makes it foolish to have ever considered not coming, and magical people here who make the thought of leaving even more foo",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Ithaka | The Poetry Foundation",
    "url": "https://www.poetryfoundation.org/poems/51296/ithaka-56d22eef917ec",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "At a Slight Angle to the Universe Advertise with Poetry Poetry Magazine Poetry Magazine Archive Submit to Poetry",
    "full_snippet": "At a Slight Angle to the Universe Advertise with Poetry Poetry Magazine Poetry Magazine Archive Submit to Poetry",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Project Gutenberg eBook #5740: Tractatus Logico-Philosophicus",
    "url": "https://www.gutenberg.org/files/5740/5740-pdf.pdf",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Shared State of Mathematics",
    "url": "https://vaibhawvipul.github.io/2025/12/30/FLT-helped-me-find-upper-planes.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Last month I wrote about failed attempt to understand Fermat\u2019s Last Theorem. I confessed failure \u2014 or at least, incomplete success. I understood the skeleton of Wiles\u2019 proof, but not its flesh. But...",
    "full_snippet": "Last month I wrote about failed attempt to understand Fermat\u2019s Last Theorem. I confessed failure \u2014 or at least, incomplete success. I understood the skeleton of Wiles\u2019 proof, but not its flesh. But something unplanned happened, I stumbled into something I wasn\u2019t looking for: a hidden architecture beneath mathematics itself. Modular Forms, Elliptic Curves, and Hyperbolic Geometry previously seemed like something I might never understand them in this lifetime. And at the center of it all, I kept finding the same object. Again and again. \u210d, the upper half-plane: where . Positive imaginary part. I started with Silverman\u2019s Rational Points on Elliptic Curves, because that\u2019s where FLT proof by Wiles had pointed initially. Elliptic curves over \u2102 are tori - quotients of the complex plane by lattices. At first this felt like a technical detail. Then I realized that every lattice is encoded by a single complex number with positive imaginary part. In other words, by a point in the upper half-plane",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "What's Our Problem? \u2014 Wait But Why",
    "url": "https://waitbutwhy.com",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "A popular long-form, stick-figure-illustrated blog about almost everything.",
    "full_snippet": "A popular long-form, stick-figure-illustrated blog about almost everything.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Petri Dish Neural Cellular Automata",
    "url": "https://pub.sakana.ai/pdnca",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "While neural cellular automata (NCA) have proven effective for modeling morphogenesis and self-organizing processes, they are typically governed by a fixed, non-adaptive update rule shared across all...",
    "full_snippet": "While neural cellular automata (NCA) have proven effective for modeling morphogenesis and self-organizing processes, they are typically governed by a fixed, non-adaptive update rule shared across all cells. Each cell applies the same learned local transition function throughout its lifetime, resulting in static developmental dynamics once training is complete. We introduce Petri Dish Neural Cellular Automata (PD-NCA), a differentiable Artificial Life substrate that removes this constraint by allowing multiple, independent NCA agents to coexist, compete, and adapt within a shared environment. Unlike conventional NCA, each agent in PD-NCA continually updates its parameters via gradient descent during the simulation itself, enabling within-lifetime learning and open-ended behavioral change. This continual, multi-agent learning process transforms morphogenesis from a fixed developmental program into a dynamic ecosystem of interacting, adaptive entities. Through these interactions, PD-NCA e",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "a journey through the american west - by Jasmine Li",
    "url": "https://jasminexli.substack.com/p/a-journey-through-the-american-west",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Taking a long journey on low-speed rail has for a while been on my bucket list.1 So, I organized a trip with friends this Thanksgiving break on the California Zephyr, which runs from Chicago to San...",
    "full_snippet": "Taking a long journey on low-speed rail has for a while been on my bucket list.1 So, I organized a trip with friends this Thanksgiving break on the California Zephyr, which runs from Chicago to San Francisco over 50+ hours. Enjoy! On Thursday, I take the bus from Ithaca to NYC, then an evening flight to Chicago, all to just reach the starting location of the Zephyr. This might be my first experience traveling exclusively to take another mode of transport. We crash at Jo\u2019s place that night, though I end up getting zero sleep \u2014 catching up, learning to juggle and do jiu-jitsu, and talking about research delightfully gets in the way! But fighting delirium from the all-nighter, we make it to Union Station and board the 2PM Zephyr on Friday with ~10 minutes to spare. The four of us taking the train together are seated in the same row on the final upper-level car. Almost immediately we are introduced on the announcement speaker to Ms. Yolanda, manager of the on-train cafe, who describes the ",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Deep-learning model predicts how fruit flies form, cell by cell | MIT News | Massachusetts Institute of Technology",
    "url": "https://news.mit.edu/2025/deep-learning-model-predicts-how-fruit-flies-form-1215",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n    Creative Commons Attribution Non-Commercial No Derivatives...",
    "full_snippet": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n    Creative Commons Attribution Non-Commercial No Derivatives license.\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided \n    below, credit the images to \"MIT.\" During early development, tissues and organs begin to bloom through the shifting, splitting, and growing of many thousands of cells. A team of MIT engineers has now developed a way to predict, minute by minute, how individual cells will fold, divide, and rearrange during a fruit fly\u2019s earliest stage of growth. The new method may one day be applied to predict the development of more complex tissues, organs, and organisms. It could also help scientists identify cell patterns that correspond to early-onset diseases, such as asthma and cancer. In a study appearing today in the journal Nature Method",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Transformers and Self-Attention (DL 19) - YouTube",
    "url": "https://www.youtube.com/watch?v=e9-0BxyKG10",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "This Davidson College lecture explores Transformer neural networks, explaining their architecture and self-attention mechanisms. Professor Bryce details how these networks process word embeddings and...",
    "full_snippet": "This Davidson College lecture explores Transformer neural networks, explaining their architecture and self-attention mechanisms. Professor Bryce details how these networks process word embeddings and leverage residual connections. The video uses diagrams to illustrate the flow of data and the role of \"query,\" \"key,\" and \"value\" layers. 0 of 26 lessons complete Follow along using the transcript. This Davidson College lecture explores Transformer neural networks, explaining their architecture and self-attention mechanisms. Professor Bryce details how these networks process word embeddings and leverage residual connections. The video uses diagrams to illustrate the flow of data and the role of \"query,\" \"key,\" and \"value\" layers. 0 of 26 lessons complete Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Towards Geometric Deep Learning",
    "url": "https://thegradient.pub/towards-geometric-deep-learning",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Geometric Deep Learning is an umbrella term for approaches considering a broad class of ML problems from the perspectives of symmetry and invariance. It provides a common blueprint allowing to derive...",
    "full_snippet": "Geometric Deep Learning is an umbrella term for approaches considering a broad class of ML problems from the perspectives of symmetry and invariance. It provides a common blueprint allowing to derive from first principles neural network architectures as diverse as CNNs, GNNs, and Transformers. Here, we study how these ideas have emerged through history from ancient Greek geometry to Graph Neural Networks. The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach \u2014 computer vision, playing Go, or protein folding \u2014 are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, ",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Consciousness and Philosophy of Mind",
    "url": "https://www.doc.ic.ac.uk/~mpsha/consciousness_and_philosophy.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Understanding consciousness is one of the most profound intellectual challenges we face, and with the advent of increasingly compelling AI, it has assumed practical signnificance. I have been working...",
    "full_snippet": "Understanding consciousness is one of the most profound intellectual challenges we face, and with the advent of increasingly compelling AI, it has assumed practical signnificance. I have been working on the topic since the mid-2000s. My conviction is that to arrive at a proper understanding of consciousness we need to engage with philosophy as well as carrying out empirical work in psychology and neuroscience. On the philosophical front, my work has been much influenced by the later writings of Wittgenstein, which I see as an antidote to our natural dualistic tendencies. The upshot is that, rather than asking what consciousness *is* (and placing a heavy metaphysical burden on that word), we should ask how words like \"consciousness\" are used. The most detailed exposition of this idea, which excavates the very roots of philosophy, is in Chapter 1 of my 2010 book \"Embodiment and the Inner Life\". But the first paper I published on the theme was \"Global Access, Embodiment, and the Conscious",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Story of Heads",
    "url": "https://lena-voita.github.io/posts/acl19_heads.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on OpenSubtitles EN-RU...",
    "full_snippet": "Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on OpenSubtitles EN-RU Model trained on OpenSubtitles EN-RU subject-> verb verb -> subject subject-> verb verb -> subject verb -> subject object -> verb verb -> object object -> verb Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on OpenSubtitles EN-RU Model trained on OpenSubtitles EN-RU Model trained on OpenSubtitles EN-RU Last updated June 12, 2025.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "World Models",
    "url": "https://worldmodels.github.io",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Can agents learn inside of their own dreams?",
    "full_snippet": "Can agents learn inside of their own dreams?",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Neural Networks, Manifolds, and Topology -- colah's blog",
    "url": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Recently, there\u2019s been a great deal of excitement and interest in deep neural networks because they\u2019ve achieved breakthrough results in areas such as computer vision.1",
    "full_snippet": "Recently, there\u2019s been a great deal of excitement and interest in deep neural networks because they\u2019ve achieved breakthrough results in areas such as computer vision.1",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How To Win",
    "url": "https://planetbanatt.net/articles/gametypes.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "I really like games. I think that they provide a nice controlled environment for distilling what it means to become good at something, and as a result I think that everybody should play them. Most...",
    "full_snippet": "I really like games. I think that they provide a nice controlled environment for distilling what it means to become good at something, and as a result I think that everybody should play them. Most games will generally have a common thread connecting them, and learning to get good at one will usually provide you with a framework that should enable you to get good at any of them. I wanted to talk about some concepts that are important for reaching a moderately high level at games in general. Specifically, I want to pull in various advice on learning games from all sorts of genres in order to illustrate the common thread that connects all of it together, even if on the surface everything seems really different. The number one reason people do not improve at games is because of self-imposed restrictions they place upon themselves. In general, it's pretty easy to figure out when things are strong very soon after playing: you will be defeated by them many times, and they will begin to reveal",
    "to_read": false,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How the NanoGPT Speedrun WR dropped by 20% in 3 months \u2014 LessWrong",
    "url": "https://www.lesswrong.com/posts/j3gp8tebQiFJqzBgg/how-the-nanogpt-speedrun-wr-dropped-by-20-in-3-months",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "In early 2024 Andrej Karpathy stood up an llm.c repo to train GPT-2 (124M), which took an equivalent of 45 minutes on 8xH100 GPUs to reach 3.28 cross entropy loss. By Jan 2025, collaborators of...",
    "full_snippet": "In early 2024 Andrej Karpathy stood up an llm.c repo to train GPT-2 (124M), which took an equivalent of 45 minutes on 8xH100 GPUs to reach 3.28 cross entropy loss. By Jan 2025, collaborators of modded-nanogpt brought that time down to 3 minutes. It sat near 3 minutes until July 2025, having a large swath of optimization already applied: RoPE, value embeddings, reduce scatter grad updates, Muon, QK Norm, Relu^2, a custom FP8 head, skip connections, flex attention, short-long windows, attention window warmup, linear lr cooldown, and more. Yet, in the last 3 months the record has fallen by another \u00a020% to 2 minutes and 20 seconds. Many of the improvements in the last 20% have not yet been published outside of the modded-nanogpt repo. This post summarizes those improvements. Not everything will generalize to larger scales, but there are some core concepts that I believe are promising. Improvements are sorted into ML and Engineering, grouped by concept, and subjectively ranked by their gene",
    "to_read": false,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How to Think About GPUs | How To Scale Your Model",
    "url": "https://jax-ml.github.io/scaling-book/gpus",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We love TPUs at Google, but GPUs are great too. This chapter takes a deep dive into the world of NVIDIA GPUs \u2013 how each chip works, how they\u2019re networked together, and what that means for LLMs,...",
    "full_snippet": "We love TPUs at Google, but GPUs are great too. This chapter takes a deep dive into the world of NVIDIA GPUs \u2013 how each chip works, how they\u2019re networked together, and what that means for LLMs, especially compared to TPUs. This section builds on Chapter 2 and Chapter 5, so you are encouraged to read them first. Jacob Austin\u2020 \u2020Google DeepMind Swapnil Patil\u2020 Adam Paszke\u2020 Reiner Pope* *MatX Aug. 18, 2025 A modern ML GPU (e.g. H100, B200) is basically a bunch of compute cores that specialize in matrix multiplication (called Streaming Multiprocessors or SMs) connected to a stick of fast memory (called HBM). Here\u2019s a diagram: Each SM, like a TPU\u2019s Tensor Core, has a dedicated matrix multiplication core (unfortunately also called a Tensor Core), a vector arithmetic unit (called a Warp Scheduler), and a fast on-chip cache (called SMEM). Unlike a TPU, which has at most 2 independent \u201cTensor Cores\u201d, a modern GPU has more than 100 SMs (132 on an H100). Each of these SMs is much less powerful than",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Bahdanau Attention Mechanism - MachineLearningMastery.com",
    "url": "https://machinelearningmastery.com/the-bahdanau-attention-mechanism",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Conventional encoder-decoder architectures for machine translation encoded every source sentence into a fixed-length vector, regardless of its length, from which the decoder would then generate a...",
    "full_snippet": "Conventional encoder-decoder architectures for machine translation encoded every source sentence into a fixed-length vector, regardless of its length, from which the decoder would then generate a translation. This made it difficult for the neural network to cope with long sentences, essentially resulting in a performance bottleneck. The Bahdanau attention was proposed to address the performance bottleneck of conventional encoder-decoder architectures, achieving significant improvements over the conventional approach. In this tutorial, you will discover the Bahdanau attention mechanism for neural machine translation. After completing this tutorial, you will know: Kick-start your project with my book Building Transformer Models with Attention. It provides self-study tutorials with working code to guide you into building a fully-working transformer model that can\ntranslate sentences from one language to another... Let\u2019s get started. The Bahdanau attention mechanism\nPhoto by Sean Oulashin,",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "KV Caching & Attention Optimization: From O(n\u00b2) to O(n) | by pdawg | Medium",
    "url": "https://medium.com/@prathamgrover777/kv-caching-attention-optimization-from-o-n%C2%B2-to-o-n-8b605f0d4072",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We\u2019ve seen how an LLM types out a thousand-word answer, word by word, as if it\u2019s \u201cthinking out loud\u201d. It feels smooth, but behind the scenes, the process is painfully inefficient. At generation step...",
    "full_snippet": "We\u2019ve seen how an LLM types out a thousand-word answer, word by word, as if it\u2019s \u201cthinking out loud\u201d. It feels smooth, but behind the scenes, the process is painfully inefficient. At generation step t, the model must ensure the next word is consistent with everything that came before: your prompt, the partial output so far, system instructions, or any hidden context. In practical terms, the model rebuilds all previous hidden states through all its transformer layers and computes Queries, Keys, and Values again, even though the previous tokens haven\u2019t changed at all. This re-computation happens layer by layer and head by head. Nothing is reused. As shown in Fig. 1, the computation per token keeps increasing with sequence length because nothing from previous steps is reused. Why This Scales So Badly? Imagine writing a paragraph, and before you add each new sentence, you re-read the entire document from the beginning. Then you do it again for the next sentence. and again. and again. that\u2019",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Transformers are Graph Neural Networks",
    "url": "https://thegradient.pub/transformers-are-graph-neural-networks",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "My engineering friends often ask me: deep learning on graphs sounds great, but are there any real applications? While Graph Neural Networks are used in recommendation systems at Pinterest, Alibaba...",
    "full_snippet": "My engineering friends often ask me: deep learning on graphs sounds great, but are there any real applications? While Graph Neural Networks are used in recommendation systems at Pinterest, Alibaba and Twitter, a more subtle success story is the Transformer architecture, which has taken the NLP world by storm. Through this post, I want to establish a link between Graph Neural Networks (GNNs) and Transformers. I'll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we can work together to drive future progress. Let's start by talking about the purpose of model architectures\u2014representation learning. At a high level, all neural network architectures build representations of input data as vectors/embeddings, which encode useful statistical and semantic information about the data. These latent or hidden representations can then be used for performing something useful, such as classifying an image ",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Using topology for discrete problems | The Borsuk-Ulam theorem and stolen necklaces - YouTube",
    "url": "https://www.youtube.com/watch?v=yuVqxCSsE7c",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Follow along using the transcript. Follow along using the transcript.",
    "full_snippet": "Follow along using the transcript. Follow along using the transcript.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Energy-Based Models \u00b7 Deep Learning",
    "url": "https://atcold.github.io/NYU-DLSP20/en/week07/07-1",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We will introduce a new framework for defining models. It provides a unifying umbrella that helps define supervised, unsupervised and self-supervised models. Energy-based models observe a set of...",
    "full_snippet": "We will introduce a new framework for defining models. It provides a unifying umbrella that helps define supervised, unsupervised and self-supervised models. Energy-based models observe a set of variables \n\ud835\udc65\nx and output a set of variables \n\ud835\udc66\ny. There are 2 major problems with feed-forward nets: Instead of trying to classify \n\ud835\udc65\nx\u2019s to \n\ud835\udc66\ny\u2019s, we would like to predict if a certain pair of (\n\ud835\udc65\nx, \n\ud835\udc66\ny) fit together or not. Or in other words, find a \n\ud835\udc66\ny compatible with \n\ud835\udc65\nx. We can also pose the problem as finding a \n\ud835\udc66\ny for which some \n\ud835\udc39\n(\n\ud835\udc65\n,\n\ud835\udc66\n)\nF(x,y) is low. For example: We define an energy function \n\ud835\udc39\n:\n\ud835\udc4b\n\u00d7\n\ud835\udc4c\n\u2192\n\ud835\udc45\nF:X\u00d7Y\u2192R where \n\ud835\udc39\n(\n\ud835\udc65\n,\n\ud835\udc66\n)\nF(x,y) describes the level of dependency between \n(\n\ud835\udc65\n,\n\ud835\udc66\n)\n(x,y) pairs. (Note that this energy is used in inference, not in learning.) The inference is given by the following equation: We would like the energy function to be smooth and differentiable so that we can use it to perform the gradient-based method for inference. I",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "On the Biology of a Large Language Model",
    "url": "https://transformer-circuits.pub/2025/attribution-graphs/biology.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We investigate the internal mechanisms used by Claude 3.5 Haiku \u2014 Anthropic's lightweight production model \u2014 in a variety of contexts, using our circuit tracing methodology.",
    "full_snippet": "We investigate the internal mechanisms used by Claude 3.5 Haiku \u2014 Anthropic's lightweight production model \u2014 in a variety of contexts, using our circuit tracing methodology.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Galaxy brain resistance",
    "url": "https://vitalik.eth.limo/general/2025/11/07/galaxybrain.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "One important property for a style of thinking and argumentation to have is what I call galaxy brain resistance: how difficult is it to abuse that style of thinking to argue for pretty much whatever...",
    "full_snippet": "One important property for a style of thinking and argumentation to have is what I call galaxy brain resistance: how difficult is it to abuse that style of thinking to argue for pretty much whatever you want - something that you already decided elsewhere for other reasons? The spirit here is similar to falsifiability in science: if your arguments can justify anything, then your arguments imply nothing. You want to get to step 2 and then stop. It's easiest to motivate the need to think about galaxy brain resistance by looking at what happens in its absence. You've probably heard many cases of people saying things like this: We are building a new decentralized ____ marketplace that will revolutionize how ____ customers interact with their providers, and will allow creators to turn their audiences into digital nation states. $____ is a governance token that lets you play a direct role in this rapidly growing market. If we capture even 1% of the ___ market share, this will be a $___ billio",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "On Seeing Through and Unseeing: The Hacker Mindset \u00b7 Gwern.net",
    "url": "https://gwern.net/unseeing",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Defining the security/hacker mindset as extreme reductionism: ignoring the surface abstractions and limitations to treat a system as a source of parts to manipulate into a different system, with...",
    "full_snippet": "Defining the security/hacker mindset as extreme reductionism: ignoring the surface abstractions and limitations to treat a system as a source of parts to manipulate into a different system, with different (and usually unintended) capabilities.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Towards a Geometric Theory of Deep Learning - Govind Menon - YouTube",
    "url": "https://www.youtube.com/watch?v=44hfoihYfJ0",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "This lecture explores the mathematical underpinnings of deep learning, focusing on the deep linear network (DLN) model. The speaker presents sharp results on training dynamics, uncovering unexpected...",
    "full_snippet": "This lecture explores the mathematical underpinnings of deep learning, focusing on the deep linear network (DLN) model. The speaker presents sharp results on training dynamics, uncovering unexpected connections to minimal surfaces and geometric invariant theory. The analysis reveals a conceptual framework for understanding \"true\" deep learning. 1 of 78 lessons complete Follow along using the transcript. This lecture explores the mathematical underpinnings of deep learning, focusing on the deep linear network (DLN) model. The speaker presents sharp results on training dynamics, uncovering unexpected connections to minimal surfaces and geometric invariant theory. The analysis reveals a conceptual framework for understanding \"true\" deep learning. 1 of 78 lessons complete Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Reinforcement Learning algorithms summarized",
    "url": "https://letters.lossfunk.com/p/reinforcement-learning-algorithms",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "The basic task of reinforcement learning is this: given a state we\u2019re in and probabilities of different actions we can take, how do we increase or decrease those probabilities so that we increase...",
    "full_snippet": "The basic task of reinforcement learning is this: given a state we\u2019re in and probabilities of different actions we can take, how do we increase or decrease those probabilities so that we increase average rewards in future. The fundamental tradeoff in this is bias, variance (as detailed below). Thanks for reading! Subscribe for free to receive new posts and support my work. The intuitive thing to do is to increase probability of actions that are \u201cbetter\u201d (in the sense of helping a higher rewarding trajectory in future) Notice that no matter which algorithm we end up using, we need to know how \u201crewarding\u201d an action is. This reward can be calculated in two ways: Montecarlo approach: Wait for full roll-outs and sum actual rewards we get in the trajectory This is high variance, low bias because you\u2019re working with actual rewards from environment (not estimates) but it has high variance because rewards for different trajectories varies by a lot Bootstrap an estimator of rewards: learn an est",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Love Song of J. Alfred Prufrock by T. S. Eliot | Poetry Magazine",
    "url": "https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "The yellow fog that rubs its back upon the window-panes, The yellow smoke that rubs its muzzle on the window-panes, Licked its tongue into the corners of the evening, Lingered upon the pools that...",
    "full_snippet": "The yellow fog that rubs its back upon the window-panes, The yellow smoke that rubs its muzzle on the window-panes, Licked its tongue into the corners of the evening, Lingered upon the pools that stand in drains, Let fall upon its back the soot that falls from chimneys, Slipped by the terrace, made\u2026",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "thalassophilia - by hannah - a lot about nothing",
    "url": "https://substack.com/inbox/post/181977520?r=294fgl",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Lately I\u2019ve been restless as ever. I miss home and all its people and real seasons where all the leaves color the sky orange-red and dance swirling in the sweeping wind. I miss the vastness of the...",
    "full_snippet": "Lately I\u2019ve been restless as ever. I miss home and all its people and real seasons where all the leaves color the sky orange-red and dance swirling in the sweeping wind. I miss the vastness of the cornfields, so thick you could drown in it \u2014 a midwestern thalassophiliac. It\u2019s the kind of empty space that you can only fill with the thoughts of leaving (in all senses of the word). It\u2019s home, the same as it was 19 years ago and the same it will be in 19 more, just as I left it. How ironic is it that Indiana is a landlocked state? Recently, I came across the term oceanic feeling, coined by Romain Rolland in a letter to Freud. He used it to describe a sensation of \u2018eternity\u2019, an unbounded union with the universe. He attributed this incredible, metaphysical state to the source of all religious energy, the universal god. Freud, in his response, dismissed this as a deformity: a vestigial sensation left over from infancy, when the borders of the world haven\u2019t yet formed, and the edges of the eg",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Concept of the Ruliad\u2014Stephen Wolfram Writings",
    "url": "https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "I call it the ruliad. Think of it as the entangled limit of everything that is computationally possible: the result of following all possible computational rules in all possible ways. It\u2019s yet...",
    "full_snippet": "I call it the ruliad. Think of it as the entangled limit of everything that is computationally possible: the result of following all possible computational rules in all possible ways. It\u2019s yet another surprising construct that\u2019s arisen from our Physics Project. And it\u2019s one that I think has extremely deep implications\u2014both in science and beyond. In many ways, the ruliad is a strange and profoundly abstract thing. But it\u2019s something very universal\u2014a kind of ultimate limit of all abstraction and generalization. And it encapsulates not only all formal possibilities but also everything about our physical universe\u2014and everything we experience can be thought of as sampling that part of the ruliad that corresponds to our particular way of perceiving and interpreting the universe. We\u2019re going to be able to say many things about the ruliad without engaging in all its technical details. (And\u2014it should be said at the outset\u2014we\u2019re still only at the very beginning of nailing down those technical de",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Francesco Capuano",
    "url": "https://fracapuano.github.io/blog/optimizing-nns",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "TLDR: A technical blog to revisit the fundamentals of what, in the crudest sense, makes Deep Learning work. A SGD-to-Muon tour, derived from first principles in math and then implemented from scratch...",
    "full_snippet": "TLDR: A technical blog to revisit the fundamentals of what, in the crudest sense, makes Deep Learning work. A SGD-to-Muon tour, derived from first principles in math and then implemented from scratch in Jax. Take a NN parametrized with parameters \n\ud835\udf03\n\u03b8. During training, the parameters are updated using differential information relating the performance obtained to the weights used, i.e. using \n\u2207\n\ud835\udc3f\n(\n\ud835\udf03\n)\n=\n\u2211\n\ud835\udc56\n\u2208\n\ud835\udc37\n\u2207\n\u2113\n\ud835\udc56\n(\n\ud835\udf03\n)\n\u2207L(\u03b8)=\u2211\ni\u2208D\n\t\u200b\n\n\u2207\u2113\ni\n\t\u200b\n\n(\u03b8), so that weights are iteratively updated according to: where \n\ud835\udc53\nf is some function of the weights \n\ud835\udf03\n\ud835\udc61\n\u2212\n1\n\u03b8\nt\u22121\n\t\u200b\n\n and gradients \n\u2207\n\ud835\udc3f\n(\n\ud835\udf03\n\ud835\udc61\n\u2212\n1\n)\n\u2207L(\u03b8\nt\u22121\n\t\u200b\n\n). For both conceptual and computational reason, one typically does not use the exact gradient of the loss \n\u2207\n\ud835\udc3f\n(\n\ud835\udf03\n)\n\u2207L(\u03b8), and rather relies on \n1\n\u2223\n\ud835\udc35\n\u2223\n\u2211\n\ud835\udc56\n\u2208\n\ud835\udc35\n\u2207\n\u2113\n\ud835\udc56\n(\n\ud835\udf03\n)\n\u2223B\u2223\n1\n\t\u200b\n\n\u2211\ni\u2208B\n\t\u200b\n\n\u2207\u2113\ni\n\t\u200b\n\n(\u03b8), referred to as the stochastic gradient for the mini-batch \n\ud835\udc35\n\u2282\n\ud835\udc37\n:\n\ud835\udc35\n\u223c\n\ud835\udc37\nB\u2282D:B\u223cD. On a conceptual level, stochastic gradients suffer less from poor i",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Just quit - Arjun Raj",
    "url": "https://arjunrajlab.substack.com/p/just-quit",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We spend a lot of time as scientists thinking about how to choose a project\u2014and that is, of course, critically important to success. But\u2026 no matter how carefully you try to pick out the most...",
    "full_snippet": "We spend a lot of time as scientists thinking about how to choose a project\u2014and that is, of course, critically important to success. But\u2026 no matter how carefully you try to pick out the most groundbreaking, innovative project imaginable, the simple truth is that not every project is going to be awesome. Consequently, just as important as the skill of choosing a project is the skill of knowing when to quit a project. In my view, we should quit far more often than we do, for the simple reason that time is so very precious. Here is possibly the scariest diagram of all time: That is not a lot of weeks. Each scientific project can take up 2-4 ENTIRE COLUMNS. As mentioned, the success of a project is way more probabilistic than we care to admit. So you have to sample, and that means rejecting many samples. Do not let this precious time go to waste. Sometimes, you just have to quit. Why is it so hard to quit in science? Here are a few top reasons, all of which are based on fear: Fear of the u",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  }
]