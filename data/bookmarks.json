[
  {
    "title": "Illicit Love Letters: Albert Camus and Maria Casares",
    "url": "https://www.theparisreview.org/blog/2018/04/11/illicit-love-letters-albert-camus-and-maria-casares",
    "date": "January 17, 2026",
    "domain": "",
    "snippet": "For the past few weeks, I\u2019ve fixated on a collection of primary source material that reads like a tidy work of epistolary fiction. It\u2019s a big book, nearly 1,300 pages, transcribed from original...",
    "full_snippet": "For the past few weeks, I\u2019ve fixated on a collection of primary source material that reads like a tidy work of epistolary fiction. It\u2019s a big book, nearly 1,300 pages, transcribed from original letters, postcards, and telegrams sent between the French philosopher and writer Albert\u00a0Camus\u00a0and the Spanish French actress Maria Casares between 1944 and 1959. It\u2019s too heavy a book to bring on the subway, so I downloaded the electronic version on my phone. My camera roll is now nearly a hundred screenshots of exchanges in French between the two lovers. The book was published in France by Gallimard and has not yet been translated into English. The romance of\u00a0Camus\u00a0and Casares is richer, if not sadder, when considered alongside the narratives of each of their work. There is an eerie doubling of life and art. Absurdity is the only certainty, and this is confirmed over and over again by coincidence and chance. The two first met on June 6, 1944, the storied day the Allied forces landed in Normandy",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "\u2018He was in mystic delirium\u2019: was this hermit mathematician a forgotten genius whose ideas could transform AI \u2013 or a lonely madman? | Mathematics | The Guardian",
    "url": "https://www.theguardian.com/science/article/2024/aug/31/alexander-grothendieck-huawei-ai-artificial-intelligence",
    "date": "January 16, 2026",
    "domain": "",
    "snippet": "In isolation, Alexander Grothendieck seemed to have lost touch with reality, but some say his metaphysical theories could contain wonders",
    "full_snippet": "In isolation, Alexander Grothendieck seemed to have lost touch with reality, but some say his metaphysical theories could contain wonders",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Mythology Of Conscious AI",
    "url": "https://www.noemamag.com/the-mythology-of-conscious-ai",
    "date": "January 15, 2026",
    "domain": "",
    "snippet": "Why consciousness is more likely a property of life than of computation and why creating conscious, or even conscious-seeming AI, is a bad idea.",
    "full_snippet": "Why consciousness is more likely a property of life than of computation and why creating conscious, or even conscious-seeming AI, is a bad idea.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Where physics and biology meet - ScienceDirect",
    "url": "https://www.sciencedirect.com/science/article/pii/S0960982224011345?via%3Dihub",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "All content on this site: Copyright \u00a9 2026 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies....",
    "full_snippet": "All content on this site: Copyright \u00a9 2026 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the relevant licensing terms apply. These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly. These cookies allow us to count visits and traffic sources so we can mea",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Colors Of Her Coat - by Scott Alexander",
    "url": "https://www.astralcodexten.com/p/the-colors-of-her-coat",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "...",
    "full_snippet": "...",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Matthew Walker's \"Why We Sleep\" Is Riddled with Scientific and Factual Errors - Alexey Guzey",
    "url": "https://guzey.com/books/why-we-sleep",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "See discussion of this essay on the forum, Hacker News (a), Marginal Revolution (a), Andrew Gelman\u2019s blog 1 (a), 2 (a), 3 (a), 4 (a), /r/slatestarcodex (a), Twitter (a), listen to BBC interviewing me...",
    "full_snippet": "See discussion of this essay on the forum, Hacker News (a), Marginal Revolution (a), Andrew Gelman\u2019s blog 1 (a), 2 (a), 3 (a), 4 (a), /r/slatestarcodex (a), Twitter (a), listen to BBC interviewing me and Walker himself about it or listen to my interview with Smart People Podcast discussing it.\n\nNote: I link to a bunch of paywalled studies in this essay. Please do not use sci-hub to access them for free and do not use this trick (a) to easily redirect papers to sci-hub.\n\nFor the clearest \u2026",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Post 51: Socratic Persuasion: Giving Opinionated Yet Truth-Seeking Advice \u2014 Neel Nanda",
    "url": "https://www.neelnanda.io/blog/51-socratic-persuasion",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "The full post is long, but you can 80/20 the value with the 700 word summary! Over half the post is eight optional case studies. Thanks to Jemima Jones, Claude 4 Opus and Gemini 2.5 Pro for help...",
    "full_snippet": "The full post is long, but you can 80/20 the value with the 700 word summary! Over half the post is eight optional case studies. Thanks to Jemima Jones, Claude 4 Opus and Gemini 2.5 Pro for help copy-editing and drafting TL;DR: I recommend giving advice\u00a0by asking questions\u00a0to walk someone through key steps in my argument\u00a0\u2014 often I\u2019m missing key info, which comes up quickly as an unexpected answer, while if I\u2019m right I\u2019m more persuasive. This error correction makes it safer to give opinionated advice, without overconfidence. This is useful in a wide range of settings, as a manager, managee, friend, and mentor, and is better for both parties, if you have the time and energy and are able to seriously engage with whether you\u00a0are wrong. Socratic Persuasion:\u00a0When trying to persuade\u00a0someone, especially if giving advice, I much prefer the Socratic method\u00a0over directly presenting my case.\u00a0I take my argument/thought process and break it down into 1-3 key step/cruxes, reframe each step into a que",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "What the humans like is responsiveness - by Sasha Chapin",
    "url": "https://sashachapin.substack.com/p/what-the-humans-like-is-responsiveness",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "What do the humans like? Apparently, they like this woman ordering food in a slightly flirtatious manner at a food truck. A total of 1.9 million souls have clicked \u201cheart\u201d on this brief clip. Okay,...",
    "full_snippet": "What do the humans like? Apparently, they like this woman ordering food in a slightly flirtatious manner at a food truck. A total of 1.9 million souls have clicked \u201cheart\u201d on this brief clip. Okay, sure\u2014on TikTok, that\u2019s not a completely unusual number. But it\u2019s a much more enthusiastic response than the other videos on this channel get, and the comments section reveals an unlikely fervor. \u201cI wish man,\u201d one guy comments, \u201cI can only wish that God would bless me with a girl like this. I\u2019d do anything for her bro.\u201d Another remark: \u201cI\u2019ve never been so in love in this app on my life.\u201d Is it that she\u2019s pretty? That can\u2019t be it entirely. I\u2019m aware of equally pretty women on the internet who try to get people to fall in love with them because it\u2019s their job\u2014influencers, porn stars, streamers, and so on. With all of their industriousness, charisma, and symmetricality, they do not get this kind of reception. And many of them are naked. What sets this person apart? It\u2019s simple: her insane respon",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Discovery fiction",
    "url": "https://michaelnotebook.com/df/index.html",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "To help me understand a scientific result, I often find it helpful to write what I call discovery fiction. By this I mean: a plausible story of how I could have discovered that result \u2013 an arc of...",
    "full_snippet": "To help me understand a scientific result, I often find it helpful to write what I call discovery fiction. By this I mean: a plausible story of how I could have discovered that result \u2013 an arc of small questions and ideas, false starts and backtracking, incremental steps eventually leading to the result. None of these things should come out of thin air: they should all be simple, almost-obvious steps. I've written a lot of discovery fiction. For instance, I have a Twitter thread on \"discovering\" the Hindu-Arabic numerals. And another Twitter thread on \"discovering\" quantum teleportation. I have discovery fiction essays about how to \"discover\" Bitcoin and how to \"discover\" the Bloom filter data structure. I've occasionally included discovery fiction inside longer books and essays. For example, section 6.2 of my first quantum computing book (jointly written with Ike Chuang) explains how the reader could plausibly have \"discovered\" the quantum search algorithm. Although it perhaps seems a",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "AI 2027",
    "url": "https://ai-2027.com",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "A research-backed AI scenario forecast.",
    "full_snippet": "A research-backed AI scenario forecast.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Intelligence Curse",
    "url": "https://intelligence-curse.ai",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "We will soon live in the intelligence age. What you do with that information will determine your place in history. The imminent arrival of AGI has pushed many to try to seize the levers of power as...",
    "full_snippet": "We will soon live in the intelligence age. What you do with that information will determine your place in history. The imminent arrival of AGI has pushed many to try to seize the levers of power as quickly as possible, leaping towards projects that, if successful, would comprehensively automate all work. There is a trillion-dollar arms race to see who can achieve such a capability first, with trillions more in gains to be won. Yes, that means you\u2019ll lose your job. But it goes beyond that: this will remove the need for regular people in our economy. Powerful actors\u2014like states and companies\u2014no longer have an incentive to care about regular people. We call this the intelligence curse. If we do nothing, the intelligence curse will work like this: But this prophecy is not yet fulfilled; we reject the view that this path is inevitable. We see a different future on the horizon, but it will require a deliberate and concerted effort to achieve it. We aim to change the incentives driving the in",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "DeepSeek's mHC: When Residual Connections Explode - Taylor Kolasinski",
    "url": "https://taylorkolasinski.com/notes/mhc-reproduction",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "Taylor Kolasinski - Engineering at FlowMode. ML systems & research, reinforcement learning, robotics. Based in Brooklyn, NY.",
    "full_snippet": "Taylor Kolasinski - Engineering at FlowMode. ML systems & research, reinforcement learning, robotics. Based in Brooklyn, NY.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "[2008.03936] Intelligent Matrix Exponentiation",
    "url": "https://arxiv.org/abs/2008.03936",
    "date": "January 12, 2026",
    "domain": "",
    "snippet": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and...",
    "full_snippet": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. arXiv Operational Status \n",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "He Co-Invented the Transformer. Now: Continuous Thought Machines [Llion Jones / Luke Darlow] - YouTube",
    "url": "https://www.youtube.com/watch?v=DtePicx_kFY",
    "date": "January 12, 2026",
    "domain": "",
    "snippet": "A Transformer inventor shifts focus, exploring novel recurrent models. This Machine Learning Street Talk episode delves into the Continuous Thought Machine, a biologically-inspired architecture....",
    "full_snippet": "A Transformer inventor shifts focus, exploring novel recurrent models. This Machine Learning Street Talk episode delves into the Continuous Thought Machine, a biologically-inspired architecture. Learn about its unique neuron design and synchronization-based representation. Follow along using the transcript. A Transformer inventor shifts focus, exploring novel recurrent models. This Machine Learning Street Talk episode delves into the Continuous Thought Machine, a biologically-inspired architecture. Learn about its unique neuron design and synchronization-based representation. Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Understanding Image Gradients ",
    "url": "https://theailearner.com/2019/05/11/understanding-image-gradients",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "In the previous blogs, we discussed different smoothing filters. Before moving forward, let\u2019s first discuss Image Gradients which will be useful in edge detection, robust feature and texture...",
    "full_snippet": "In the previous blogs, we discussed different smoothing filters. Before moving forward, let\u2019s first discuss Image Gradients which will be useful in edge detection, robust feature and texture matching. So, let\u2019s first recall what a gradient is. In mathematics, the term gradient of a function means how a function is changing wrt. its arguments or independent variables. The gradient term is more frequently used for multi-variable functions. For a single variable function, we refer to this as the slope. The\u00a0gradient\u00a0of an N-variable function at each point is an N-D\u00a0vector\u00a0with the components given by the\u00a0derivatives\u00a0in the N-directions. e.g. for a 3-variable function (f(x,y,z)), the gradient, if it exists, is given by Thus, the gradient provides two pieces of information \u2013 magnitude and direction. The direction of the gradient tells us the direction of greatest increase while the magnitude represents the rate of increase in that direction. Because gradients are defined only for continuous ",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": " \"Autoregressive Transformers vs Text Diffusion Models\" / X",
    "url": "https://x.com/rasbt/status/2010376305720594810",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "To view keyboard shortcuts, press question mark\nView keyboard shortcuts\nAdded to your Bookmarks\nAdd to Folder\nHome\nExplore\nNotifications\nChat\nGrok\nBookmarks\nCreator...",
    "full_snippet": "To view keyboard shortcuts, press question mark\nView keyboard shortcuts\nAdded to your Bookmarks\nAdd to Folder\nHome\nExplore\nNotifications\nChat\nGrok\nBookmarks\nCreator Studio\nCommunities\nPremium\nProfile\nMore\nPost\nweeye\n@weeyev\nArticle\nReply\nSee new posts\nConversation\nSebastian Raschka\n@rasbt\nAutoregressive Transformers vs Text Diffusion Models\n3\n4\n31\n1.2K\nSomething I've been asked a lot in recent weeks is whether we see alternatives to the autoregressive transformer architecture (standard LLMs) in 2026. For now, I strongly believe that the transformer is here to stay for state-of-the-art modeling performance (for at least one to a few more years).\nBut yes, it will change a bit. Towards the end of the year, we saw a stronger focus towards hybrid architectures and making it more efficient. This is, of course, not a new idea, but recent releases from flagship labs show that there's now a stronger focus on it. (E.g., see Qwen3-Next, Kimi Linear, Nvidia Nemotron 3, DeepSeek V3.2 with sparse at",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Don't fall into the anti-AI hype - <antirez>",
    "url": "https://antirez.com/news/158",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "<antirez>\nDon't fall into the anti-AI hype\nantirez 5 hours ago. 36621 views.\nI love writing software, line by line. It could be said that my career was a continuous effort to create software well...",
    "full_snippet": "<antirez>\nDon't fall into the anti-AI hype\nantirez 5 hours ago. 36621 views.\nI love writing software, line by line. It could be said that my career was a continuous effort to create software well written, minimal, where the human touch was the fundamental feature. I also hope for a society where the last are not forgotten. Moreover, I don't want AI to economically succeed, I don't care if the current economic system is subverted (I could be very happy, honestly, if it goes in the direction of a massive redistribution of wealth). But, I would not respect myself and my intelligence if my idea of software and society would impair my vision: facts are facts, and AI is going to change programming forever.\n\nIn 2020 I left my job in order to write a novel about AI, universal basic income, a society that adapted to the automation of work facing many challenges. At the very end of 2024 I opened a YouTube channel focused on AI, its use in coding tasks, its potential social and economical effects",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "First-order Derivative kernels for Edge Detection | TheAILearner",
    "url": "https://theailearner.com/2019/05/24/first-order-derivative-kernels-for-edge-detection",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "Remember that derivatives only exists for continuous functions but the image is a discrete 2D light intensity function. Thus in the last blog, we approximated the image gradients using finite...",
    "full_snippet": "Remember that derivatives only exists for continuous functions but the image is a discrete 2D light intensity function. Thus in the last blog, we approximated the image gradients using finite approximation as For the edge detection case, we will prefer the central difference as shown above. Using this central difference, we can obtain the derivative filter in x and y directions as shown below Here, we have assumed that the\u00a0x-coordinate is increasing in the \u201cright\u201d-direction, and\u00a0y-coordinate in the \u201cdown\u201d-direction. By weighting these x and y derivatives, we can obtain different edge detection filters. Let\u2019s see how This is obtained by multiplying the x, and y-derivative filters obtained above with some smoothing filter(1D) in the other direction. For example, a 3\u00d73 Sobel-x and Sobel-y filter can be obtained as As we know that the Gaussian filter is used for blurring thus, the Sobel operator computes the gradient with smoothing. Thus this is less sensitive to noise. Because of separabi",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Using AI, Mathematicians Find Hidden Glitches in Fluid Equations | Quanta Magazine",
    "url": "https://www.quantamagazine.org/using-ai-mathematicians-find-hidden-glitches-in-fluid-equations-20260109",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "Nearly 200 years ago, the physicists Claude-Louis Navier and George Gabriel Stokes put the finishing touches on a set of equations that describe how fluids swirl. And for nearly 200 years, the...",
    "full_snippet": "Nearly 200 years ago, the physicists Claude-Louis Navier and George Gabriel Stokes put the finishing touches on a set of equations that describe how fluids swirl. And for nearly 200 years, the Navier-Stokes equations have served as an unimpeachable theory of how fluids in the real world behave \u2014 from ocean currents threading their way between the continents to air wrapping around an aircraft\u2019s wings. Nevertheless, many mathematicians suspect that glitches hide deep within the equations. They have a hunch that in certain situations, the theory fails. In these cases, the equations will predict a fluid moving in some unphysical, incomprehensible way \u2014 spinning into an impossibly fast vortex, for instance, or instantly reversing its flow. Some quantity in the equations will grow infinitely large, or \u201cblow up,\u201d as mathematicians put it. Despite immense effort, no one has been able to come up with a situation where the Navier-Stokes equations falter. Doing so \u2014 or, alternatively, proving tha",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Genie: Generative Interactive Environments",
    "url": "https://arxiv.org/pdf/2402.15391",
    "date": "January 8, 2026",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Chandrasekhar\u2019s Voyage into Black Holes",
    "url": "https://mathsciencehistory.com/flashcards-friday-chandrasekhars-voyage-into-black-holes",
    "date": "January 8, 2026",
    "domain": "",
    "snippet": "Wel\u00adcome to Flash\u00adcards Fri\u00adday here at Math! Sci\u00adence! His\u00adto\u00adry! where every Fri\u00adday, we take a lit\u00adtle idea and make a big dis\u00adcov\u00adery out of\u00a0it. I\u2019m your host, Gabrielle Bir\u00adchak, and today\u2019s...",
    "full_snippet": "Wel\u00adcome to Flash\u00adcards Fri\u00adday here at Math! Sci\u00adence! His\u00adto\u00adry! where every Fri\u00adday, we take a lit\u00adtle idea and make a big dis\u00adcov\u00adery out of\u00a0it. I\u2019m your host, Gabrielle Bir\u00adchak, and today\u2019s sto\u00adry is about a young sci\u00aden\u00adtist, a long, relax\u00ading, boat ride, and a rev\u00ade\u00adla\u00adtion that changed the way we under\u00adstand the death of stars, and the birth of black\u00a0holes. The year was 1930. A young man named Sub\u00adrah\u00admanyan Chan\u00addrasekhar, just 19 years old, was board\u00ading a ship called the S.S. Pil\u00adsna in Bom\u00adbay, India, head\u00aded for England. The voy\u00adage would take about two weeks, a slow cross\u00ading of the Indi\u00adan Ocean, through the Suez Canal, across the Mediter\u00adranean, and final\u00adly toward the Eng\u00adlish Chan\u00adnel.\nNo phones. No inter\u00adnet. Just the ocean, his books, a note\u00adbook, and a whole lot of\u00a0time. Chan\u00addrasekhar wasn\u2019t plan\u00adning on rev\u00ado\u00adlu\u00adtion\u00adiz\u00ading astro\u00adphysics on this trip.\nHe sim\u00adply want\u00aded to get to Cam\u00adbridge Uni\u00adver\u00adsi\u00adty, where he\u2019d begin his grad\u00adu\u00adate studies. But some\u00adwhere b",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Thinking About a Bias for Better Semantic Coherence and Reuse - jchencxh",
    "url": "https://jchencxh.github.io/blog/better-coherence",
    "date": "January 8, 2026",
    "domain": "",
    "snippet": "For a general intelligence, good generalisation requires good reuse of semantic components. Good reuse of semantics requires that there exists a consistent internal handle for the same semantic...",
    "full_snippet": "For a general intelligence, good generalisation requires good reuse of semantic components. Good reuse of semantics requires that there exists a consistent internal handle for the same semantic purpose across contexts (e.g., a stable readout for classification from a linear layer or stable causal role in the computation) so that downstream computations can depend on \u201cthe same thing\u201d reliably even if the internal basis is distributed or not uniquely identifiable. This typically requires some notion of (conditional) invariance or equivariance, which could be that the concept\u2019s identity should be stable under semantics-preserving transformations, and when a transformation should matter (e.g., location), the representation should change in a predictable, structured way. Thus good reuse of semantic components requires that the components themselves have a notion of semantic invariance and are likely to have relationships that reflect the underlying reality it observes. I\u2019m going to refer to",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How not to do research - Rajan Agarwal",
    "url": "https://www.rajan.sh/multiplayer",
    "date": "January 8, 2026",
    "domain": "",
    "snippet": "I don't usually share when things go wrong. Like most, my public work tends to be the stuff that worked, but I learned a lot from this project, and I want to share what I learned about the problem,...",
    "full_snippet": "I don't usually share when things go wrong. Like most, my public work tends to be the stuff that worked, but I learned a lot from this project, and I want to share what I learned about the problem, and how not to do research. I tried to train a Genie-style world model that could learn to segment two players and their independent action spaces from Pong video alone, without labels or hardcoded structure. The model kept collapsing to degenerate solutions. It would always track the ball as \"player 1\" or even treat the score as an agent. I eventually realized I was trying to force behavior, ignoring the core problem that unsupervised multi-actor discovery requires data diversity and scale. With only one game type and a single camera angle, there's no modelling pressure to learn the \"right\" segmentation. Any learned decomposition that enables good reconstruction is equally valid. But I should have known this before I wrote any training code. I spent weeks adding designing custom loss functi",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Braitenberg vehicle - Wikipedia",
    "url": "https://en.wikipedia.org/wiki/Braitenberg_vehicle",
    "date": "January 7, 2026",
    "domain": "",
    "snippet": "A Braitenberg vehicle is a concept presented as a thought experiment by the Italian cyberneticist Valentino Braitenberg in his book Vehicles: Experiments in Synthetic Psychology. The book models the...",
    "full_snippet": "A Braitenberg vehicle is a concept presented as a thought experiment by the Italian cyberneticist Valentino Braitenberg in his book Vehicles: Experiments in Synthetic Psychology. The book models the animal world in a minimalistic and constructive way, from simple reactive behaviours (like phototaxis) through the simplest vehicles, to the formation of concepts, spatial behaviour, and generation of ideas. For the simplest vehicles, the motion of the vehicle is directly controlled by some sensors (for example photo cells). Yet the resulting behaviour may appear complex or even intelligent. A Braitenberg vehicle is an agent that can autonomously move around based on its sensor inputs. It has primitive sensors that measure some stimulus at a point, and wheels (each driven by its own motor) that function as actuators or effectors. In the simplest configuration, a sensor is directly connected to an effector, so that a sensed signal immediately produces a movement of the wheel. Depending on ho",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Radial Basis Function ",
    "url": "https://www.appliedaicourse.com/blog/radial-basis-function-in-machine-learning",
    "date": "January 7, 2026",
    "domain": "",
    "snippet": "Radial Basis Functions (RBF) play an essential role in Machine Learning, particularly in addressing non-linear problems. They are used to approximate complex functions, classify data, and solve...",
    "full_snippet": "Radial Basis Functions (RBF) play an essential role in Machine Learning, particularly in addressing non-linear problems. They are used to approximate complex functions, classify data, and solve regression tasks efficiently. RBFs became popular in the late 1980s when Broomhead and Lowe introduced RBF Neural Networks, offering a new way to handle non-linear relationships in data. Since then, RBF networks have been widely applied in areas like pattern recognition, time-series prediction, and control systems. In this article, we will break down the concept of Radial Basis Functions, their architecture, training process, and applications in a simple, beginner-friendly way. A Radial Basis Function (RBF) is a type of mathematical function whose value depends only on the distance from a central point. In Machine Learning, RBFs are commonly used to model non-linear relationships, making them effective for tasks like function approximation, classification, and regression. The Radial Basis Functi",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "REAP- model pruning ",
    "url": "https://www.cerebras.ai/blog/reap",
    "date": "January 5, 2026",
    "domain": "",
    "snippet": "Cerebras Systems and U.S. Department of Energy Sign MOU to Accelerate the Genesis Mission and U.S. National AI Initiative. Read more >> OCT 16 2025 TL;DR: We introduce REAP (Router-weighted Expert...",
    "full_snippet": "Cerebras Systems and U.S. Department of Energy Sign MOU to Accelerate the Genesis Mission and U.S. National AI Initiative. Read more >> OCT 16 2025 TL;DR: We introduce REAP (Router-weighted Expert Activation Pruning), a new one-shot method for compressing Mixture-of-Experts (MoE) language models. Our key finding is that for generative tasks like code generation pruning low-impact experts is fundamentally better than merging them. REAP removes up to 50% of experts from models as large as 1 trillion parameters while largely maintaining baseline model quality. For instance, with the\u00a0Qwen3-480B-Coder-FP8\u00a0model, REAP at 50% pruning retains 97.6% of its baseline non-agentic coding ability and 96.7% on the agentic SWE-Bench benchmark. We are open-sourcing the complete codebase and pruned model checkpoints on HuggingFace to encourage further research. Sparsely-activated Mixture-of-Experts (SMoE) models achieve their high quality by\u00a0decoupling their total parameter count from their computationa",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Hidden Mathematics in Stranger Things - YouTube",
    "url": "https://www.youtube.com/watch?v=_Y0smObqqhw",
    "date": "January 4, 2026",
    "domain": "",
    "snippet": "Ellie Sleightholm explores the mathematical concepts in Stranger Things 5. The video analyzes Dustin's circle calculations and delves into complex equations from general relativity. Learn about the...",
    "full_snippet": "Ellie Sleightholm explores the mathematical concepts in Stranger Things 5. The video analyzes Dustin's circle calculations and delves into complex equations from general relativity. Learn about the ADM split and Einstein's field equations, as they relate to wormholes. Follow along using the transcript. Ellie Sleightholm explores the mathematical concepts in Stranger Things 5. The video analyzes Dustin's circle calculations and delves into complex equations from general relativity. Learn about the ADM split and Einstein's field equations, as they relate to wormholes. Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Visualizing transformers and attention | Talk for TNG Big Tech Day '24 - YouTube",
    "url": "https://www.youtube.com/watch?v=KJtZARuO3JY",
    "date": "January 4, 2026",
    "domain": "",
    "snippet": "Follow along using the transcript. Follow along using the transcript. New New New New ",
    "full_snippet": "Follow along using the transcript. Follow along using the transcript. New New New New ",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "\u2018grokking (NN)\u2019 directory \u00b7 Gwern.net",
    "url": "https://gwern.net/doc/ai/scaling/emergence/grokking/index",
    "date": "January 3, 2026",
    "domain": "",
    "snippet": "Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond The slingshot helps with learning Emergent properties with repeated...",
    "full_snippet": "Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond The slingshot helps with learning Emergent properties with repeated examples Grokking Modular Polynomials Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks Grokfast: Accelerated Grokking by Amplifying Slow Gradients Deep Grokking: Would Deep Neural Networks Generalize Better? Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition A Tale of Tails: Model Collapse as a Change of Scaling Laws Critical Data Size of Language Models from a Grokking Perspective Grokking Group Multiplication with Cosets Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization Grokking Bey",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Claude Bliss Attractor - by Scott Alexander",
    "url": "https://www.astralcodexten.com/p/the-claude-bliss-attractor",
    "date": "January 2, 2026",
    "domain": "",
    "snippet": "This is a reported phenomenon where if two copies of Claude talk to each other, they end up spiraling into rapturous discussion of spiritual bliss, Buddhism, and the nature of consciousness. From the...",
    "full_snippet": "This is a reported phenomenon where if two copies of Claude talk to each other, they end up spiraling into rapturous discussion of spiritual bliss, Buddhism, and the nature of consciousness. From the system card: Anthropic swears they didn\u2019t do this on purpose; when they ask Claude why this keeps happening, Claude can\u2019t explain. Needless to say, this has made lots of people freak out / speculate wildly. I think there are already a few good partial explanations of this (especially Nostalgebraist here), but they deserve to be fleshed out and spread more fully. Let\u2019s start with an easier question: why do games of Chinese whispers with AI art usually end with monstrous caricatures of black people? AFAICT this was first discovered by Gene Kogan, who started with the Distracted Boyfriend meme and asked ChatGPT to \u201cgenerate the same photo 5 seconds in the future\u201d hundreds of times: At first, this worked as expected, generating (slightly distorted) scenes of how the Distracted Boyfriend situat",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures",
    "url": "https://arxiv.org/pdf/2402.15332",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "John Carmack on Idea Generation",
    "url": "https://amasad.me/carmack",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "Last year at an internal talk at Facebook I was fortunate to see [John Carmack](https://en.wikipedia.org/wiki/John_Carmack) speak about his idea generation system. At first I was disappointed...",
    "full_snippet": "Last year at an internal talk at Facebook I was fortunate to see [John Carmack](https://en.wikipedia.org/wiki/John_Carmack) speak about his idea generation system. At first I was disappointed because...",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Category: The Essence of Composition | \u00a0\u00a0Bartosz Milewski's Programming Cafe",
    "url": "https://bartoszmilewski.com/2014/11/04/category-the-essence-of-composition",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "I was overwhelmed by the positive response to my previous post, the Preface to Category Theory for Programmers. At the same time, it scared the heck out of me because I realized what high...",
    "full_snippet": "I was overwhelmed by the positive response to my previous post, the Preface to Category Theory for Programmers. At the same time, it scared the heck out of me because I realized what high expectations people were placing in me. I\u2019m afraid that no matter what I\u2019ll write, a lot of readers will be disappointed. Some readers would like the book to be more practical, others more abstract. Some hate C++ and would like all examples in Haskell, others hate Haskell and demand examples in Java. And I know that the pace of exposition will be too slow for some and too fast for others. This will not be the perfect book. It will be a compromise. All I can hope is that I\u2019ll be able to share some of my aha! moments with my readers. Let\u2019s start with the basics. A category is an embarrassingly simple concept. A category consists of objects and arrows that go between them. That\u2019s why categories are so easy to represent pictorially. An object can be drawn as a circle or a point, and an arrow\u2026 is an arrow.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Mixture of Experts Explained",
    "url": "https://huggingface.co/blog/moe",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "With the release of Mixtral 8x7B (announcement, model card), a class of transformer has become the hottest topic in the open AI community: Mixture of Experts, or MoEs for short. In this blog post, we...",
    "full_snippet": "With the release of Mixtral 8x7B (announcement, model card), a class of transformer has become the hottest topic in the open AI community: Mixture of Experts, or MoEs for short. In this blog post, we take a look at the building blocks of MoEs, how they\u2019re trained, and the tradeoffs to consider when serving them for inference. Let\u2019s dive in! MoEs: Let\u2019s dive in! The scale of a model is one of the most important axes for better model quality. Given a fixed computing budget, training a larger model for fewer steps is better than training a smaller model for more steps. Mixture of Experts enable models to be pretrained with far less compute, which means you can dramatically scale up the model or dataset size with the same compute budget as a dense model. In particular, a MoE model should achieve the same quality as its dense counterpart much faster during pretraining. So, what exactly is a MoE? In the context of transformer models, a MoE consists of two main elements: So, to recap, in MoEs",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Futurist Manifesto, by Filippo Tommaso Marinetti",
    "url": "https://www.arthistoryproject.com/artists/filippo-tommaso-marinetti/the-futurist-manifesto",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "We had stayed up all night, my friends and I, under hanging mosque lamps with domes of filigreed brass, domes starred like our spirits, shining like them with the prisoned radiance of electric...",
    "full_snippet": "We had stayed up all night, my friends and I, under hanging mosque lamps with domes of filigreed brass, domes starred like our spirits, shining like them with the prisoned radiance of electric hearts. For hours we had trampled our atavistic ennui into rich oriental rugs, arguing up to the last confines of logic and blackening many reams of paper with our frenzied scribbling. An immense pride was buoying us up, because we felt ourselves alone at that hour, alone, awake, and on our feet, like proud beacons or forward sentries against an army of hostile stars glaring down at us from their celestial encampments. Alone with stokers feeding the hellish fires of great ships, alone with the black spectres who grope in the red-hot bellies of locomotives launched on their crazy courses, alone with drunkards reeling like wounded birds along the city walls. Suddenly we jumped, hearing the mighty noise of the huge double-decker trams that rumbled by outside, ablaze with colored lights, like village",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "returning to stanford | writing",
    "url": "https://masonjwang.com/writing/returning-to-stanford",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "Every reinvention story is a confession, and mine is this: before I started at Stanford this September, when I had often thought I would never return, I was very dismissive of what college had to...",
    "full_snippet": "Every reinvention story is a confession, and mine is this: before I started at Stanford this September, when I had often thought I would never return, I was very dismissive of what college had to offer. I had two gap years in my pockets: living in Rome with other founders; crying after firing someone for the first time; traveling the world, hiking in Uzbekistan, meeting my cofounder's family in Abu Dhabi; sneaking into Stanford dining halls. While I never admitted it, my mindset reeked of how proudly I had weaponized myself. My plan was to do one quarter at Stanford, learn the math that was bottlenecking me, methodically explore, then find a \u201crocketship\u201d to leave for. I finished my first quarter last week. It rushes past the same way: biking through campus feeling like a kid, laughing with new friends harder than I have in my life. I have found a magical place that makes it foolish to have ever considered not coming, and magical people here who make the thought of leaving even more foo",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Ithaka | The Poetry Foundation",
    "url": "https://www.poetryfoundation.org/poems/51296/ithaka-56d22eef917ec",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "At a Slight Angle to the Universe Advertise with Poetry Poetry Magazine Poetry Magazine Archive Submit to Poetry",
    "full_snippet": "At a Slight Angle to the Universe Advertise with Poetry Poetry Magazine Poetry Magazine Archive Submit to Poetry",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Project Gutenberg eBook #5740: Tractatus Logico-Philosophicus",
    "url": "https://www.gutenberg.org/files/5740/5740-pdf.pdf",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Shared State of Mathematics",
    "url": "https://vaibhawvipul.github.io/2025/12/30/FLT-helped-me-find-upper-planes.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Last month I wrote about failed attempt to understand Fermat\u2019s Last Theorem. I confessed failure \u2014 or at least, incomplete success. I understood the skeleton of Wiles\u2019 proof, but not its flesh. But...",
    "full_snippet": "Last month I wrote about failed attempt to understand Fermat\u2019s Last Theorem. I confessed failure \u2014 or at least, incomplete success. I understood the skeleton of Wiles\u2019 proof, but not its flesh. But something unplanned happened, I stumbled into something I wasn\u2019t looking for: a hidden architecture beneath mathematics itself. Modular Forms, Elliptic Curves, and Hyperbolic Geometry previously seemed like something I might never understand them in this lifetime. And at the center of it all, I kept finding the same object. Again and again. \u210d, the upper half-plane: where . Positive imaginary part. I started with Silverman\u2019s Rational Points on Elliptic Curves, because that\u2019s where FLT proof by Wiles had pointed initially. Elliptic curves over \u2102 are tori - quotients of the complex plane by lattices. At first this felt like a technical detail. Then I realized that every lattice is encoded by a single complex number with positive imaginary part. In other words, by a point in the upper half-plane",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "What's Our Problem? \u2014 Wait But Why",
    "url": "https://waitbutwhy.com",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "A popular long-form, stick-figure-illustrated blog about almost everything.",
    "full_snippet": "A popular long-form, stick-figure-illustrated blog about almost everything.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Petri Dish Neural Cellular Automata",
    "url": "https://pub.sakana.ai/pdnca",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "While neural cellular automata (NCA) have proven effective for modeling morphogenesis and self-organizing processes, they are typically governed by a fixed, non-adaptive update rule shared across all...",
    "full_snippet": "While neural cellular automata (NCA) have proven effective for modeling morphogenesis and self-organizing processes, they are typically governed by a fixed, non-adaptive update rule shared across all cells. Each cell applies the same learned local transition function throughout its lifetime, resulting in static developmental dynamics once training is complete. We introduce Petri Dish Neural Cellular Automata (PD-NCA), a differentiable Artificial Life substrate that removes this constraint by allowing multiple, independent NCA agents to coexist, compete, and adapt within a shared environment. Unlike conventional NCA, each agent in PD-NCA continually updates its parameters via gradient descent during the simulation itself, enabling within-lifetime learning and open-ended behavioral change. This continual, multi-agent learning process transforms morphogenesis from a fixed developmental program into a dynamic ecosystem of interacting, adaptive entities. Through these interactions, PD-NCA e",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "a journey through the american west - by Jasmine Li",
    "url": "https://jasminexli.substack.com/p/a-journey-through-the-american-west",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Taking a long journey on low-speed rail has for a while been on my bucket list.1 So, I organized a trip with friends this Thanksgiving break on the California Zephyr, which runs from Chicago to San...",
    "full_snippet": "Taking a long journey on low-speed rail has for a while been on my bucket list.1 So, I organized a trip with friends this Thanksgiving break on the California Zephyr, which runs from Chicago to San Francisco over 50+ hours. Enjoy! On Thursday, I take the bus from Ithaca to NYC, then an evening flight to Chicago, all to just reach the starting location of the Zephyr. This might be my first experience traveling exclusively to take another mode of transport. We crash at Jo\u2019s place that night, though I end up getting zero sleep \u2014 catching up, learning to juggle and do jiu-jitsu, and talking about research delightfully gets in the way! But fighting delirium from the all-nighter, we make it to Union Station and board the 2PM Zephyr on Friday with ~10 minutes to spare. The four of us taking the train together are seated in the same row on the final upper-level car. Almost immediately we are introduced on the announcement speaker to Ms. Yolanda, manager of the on-train cafe, who describes the ",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Deep-learning model predicts how fruit flies form, cell by cell | MIT News | Massachusetts Institute of Technology",
    "url": "https://news.mit.edu/2025/deep-learning-model-predicts-how-fruit-flies-form-1215",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n    Creative Commons Attribution Non-Commercial No Derivatives...",
    "full_snippet": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n    Creative Commons Attribution Non-Commercial No Derivatives license.\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided \n    below, credit the images to \"MIT.\" During early development, tissues and organs begin to bloom through the shifting, splitting, and growing of many thousands of cells. A team of MIT engineers has now developed a way to predict, minute by minute, how individual cells will fold, divide, and rearrange during a fruit fly\u2019s earliest stage of growth. The new method may one day be applied to predict the development of more complex tissues, organs, and organisms. It could also help scientists identify cell patterns that correspond to early-onset diseases, such as asthma and cancer. In a study appearing today in the journal Nature Method",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Transformers and Self-Attention (DL 19) - YouTube",
    "url": "https://www.youtube.com/watch?v=e9-0BxyKG10",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "This Davidson College lecture explores Transformer neural networks, explaining their architecture and self-attention mechanisms. Professor Bryce details how these networks process word embeddings and...",
    "full_snippet": "This Davidson College lecture explores Transformer neural networks, explaining their architecture and self-attention mechanisms. Professor Bryce details how these networks process word embeddings and leverage residual connections. The video uses diagrams to illustrate the flow of data and the role of \"query,\" \"key,\" and \"value\" layers. 0 of 26 lessons complete Follow along using the transcript. This Davidson College lecture explores Transformer neural networks, explaining their architecture and self-attention mechanisms. Professor Bryce details how these networks process word embeddings and leverage residual connections. The video uses diagrams to illustrate the flow of data and the role of \"query,\" \"key,\" and \"value\" layers. 0 of 26 lessons complete Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Towards Geometric Deep Learning",
    "url": "https://thegradient.pub/towards-geometric-deep-learning",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Geometric Deep Learning is an umbrella term for approaches considering a broad class of ML problems from the perspectives of symmetry and invariance. It provides a common blueprint allowing to derive...",
    "full_snippet": "Geometric Deep Learning is an umbrella term for approaches considering a broad class of ML problems from the perspectives of symmetry and invariance. It provides a common blueprint allowing to derive from first principles neural network architectures as diverse as CNNs, GNNs, and Transformers. Here, we study how these ideas have emerged through history from ancient Greek geometry to Graph Neural Networks. The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach \u2014 computer vision, playing Go, or protein folding \u2014 are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, ",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Consciousness and Philosophy of Mind",
    "url": "https://www.doc.ic.ac.uk/~mpsha/consciousness_and_philosophy.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Understanding consciousness is one of the most profound intellectual challenges we face, and with the advent of increasingly compelling AI, it has assumed practical signnificance. I have been working...",
    "full_snippet": "Understanding consciousness is one of the most profound intellectual challenges we face, and with the advent of increasingly compelling AI, it has assumed practical signnificance. I have been working on the topic since the mid-2000s. My conviction is that to arrive at a proper understanding of consciousness we need to engage with philosophy as well as carrying out empirical work in psychology and neuroscience. On the philosophical front, my work has been much influenced by the later writings of Wittgenstein, which I see as an antidote to our natural dualistic tendencies. The upshot is that, rather than asking what consciousness *is* (and placing a heavy metaphysical burden on that word), we should ask how words like \"consciousness\" are used. The most detailed exposition of this idea, which excavates the very roots of philosophy, is in Chapter 1 of my 2010 book \"Embodiment and the Inner Life\". But the first paper I published on the theme was \"Global Access, Embodiment, and the Conscious",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Story of Heads",
    "url": "https://lena-voita.github.io/posts/acl19_heads.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on OpenSubtitles EN-RU...",
    "full_snippet": "Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on OpenSubtitles EN-RU Model trained on OpenSubtitles EN-RU subject-> verb verb -> subject subject-> verb verb -> subject verb -> subject object -> verb verb -> object object -> verb Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on OpenSubtitles EN-RU Model trained on OpenSubtitles EN-RU Model trained on OpenSubtitles EN-RU Last updated June 12, 2025.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "World Models",
    "url": "https://worldmodels.github.io",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Can agents learn inside of their own dreams?",
    "full_snippet": "Can agents learn inside of their own dreams?",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Neural Networks, Manifolds, and Topology -- colah's blog",
    "url": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Recently, there\u2019s been a great deal of excitement and interest in deep neural networks because they\u2019ve achieved breakthrough results in areas such as computer vision.1",
    "full_snippet": "Recently, there\u2019s been a great deal of excitement and interest in deep neural networks because they\u2019ve achieved breakthrough results in areas such as computer vision.1",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How To Win",
    "url": "https://planetbanatt.net/articles/gametypes.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "I really like games. I think that they provide a nice controlled environment for distilling what it means to become good at something, and as a result I think that everybody should play them. Most...",
    "full_snippet": "I really like games. I think that they provide a nice controlled environment for distilling what it means to become good at something, and as a result I think that everybody should play them. Most games will generally have a common thread connecting them, and learning to get good at one will usually provide you with a framework that should enable you to get good at any of them. I wanted to talk about some concepts that are important for reaching a moderately high level at games in general. Specifically, I want to pull in various advice on learning games from all sorts of genres in order to illustrate the common thread that connects all of it together, even if on the surface everything seems really different. The number one reason people do not improve at games is because of self-imposed restrictions they place upon themselves. In general, it's pretty easy to figure out when things are strong very soon after playing: you will be defeated by them many times, and they will begin to reveal",
    "to_read": false,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How the NanoGPT Speedrun WR dropped by 20% in 3 months \u2014 LessWrong",
    "url": "https://www.lesswrong.com/posts/j3gp8tebQiFJqzBgg/how-the-nanogpt-speedrun-wr-dropped-by-20-in-3-months",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "In early 2024 Andrej Karpathy stood up an llm.c repo to train GPT-2 (124M), which took an equivalent of 45 minutes on 8xH100 GPUs to reach 3.28 cross entropy loss. By Jan 2025, collaborators of...",
    "full_snippet": "In early 2024 Andrej Karpathy stood up an llm.c repo to train GPT-2 (124M), which took an equivalent of 45 minutes on 8xH100 GPUs to reach 3.28 cross entropy loss. By Jan 2025, collaborators of modded-nanogpt brought that time down to 3 minutes. It sat near 3 minutes until July 2025, having a large swath of optimization already applied: RoPE, value embeddings, reduce scatter grad updates, Muon, QK Norm, Relu^2, a custom FP8 head, skip connections, flex attention, short-long windows, attention window warmup, linear lr cooldown, and more. Yet, in the last 3 months the record has fallen by another \u00a020% to 2 minutes and 20 seconds. Many of the improvements in the last 20% have not yet been published outside of the modded-nanogpt repo. This post summarizes those improvements. Not everything will generalize to larger scales, but there are some core concepts that I believe are promising. Improvements are sorted into ML and Engineering, grouped by concept, and subjectively ranked by their gene",
    "to_read": false,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How to Think About GPUs | How To Scale Your Model",
    "url": "https://jax-ml.github.io/scaling-book/gpus",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We love TPUs at Google, but GPUs are great too. This chapter takes a deep dive into the world of NVIDIA GPUs \u2013 how each chip works, how they\u2019re networked together, and what that means for LLMs,...",
    "full_snippet": "We love TPUs at Google, but GPUs are great too. This chapter takes a deep dive into the world of NVIDIA GPUs \u2013 how each chip works, how they\u2019re networked together, and what that means for LLMs, especially compared to TPUs. This section builds on Chapter 2 and Chapter 5, so you are encouraged to read them first. Jacob Austin\u2020 \u2020Google DeepMind Swapnil Patil\u2020 Adam Paszke\u2020 Reiner Pope* *MatX Aug. 18, 2025 A modern ML GPU (e.g. H100, B200) is basically a bunch of compute cores that specialize in matrix multiplication (called Streaming Multiprocessors or SMs) connected to a stick of fast memory (called HBM). Here\u2019s a diagram: Each SM, like a TPU\u2019s Tensor Core, has a dedicated matrix multiplication core (unfortunately also called a Tensor Core), a vector arithmetic unit (called a Warp Scheduler), and a fast on-chip cache (called SMEM). Unlike a TPU, which has at most 2 independent \u201cTensor Cores\u201d, a modern GPU has more than 100 SMs (132 on an H100). Each of these SMs is much less powerful than",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Bahdanau Attention Mechanism - MachineLearningMastery.com",
    "url": "https://machinelearningmastery.com/the-bahdanau-attention-mechanism",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Conventional encoder-decoder architectures for machine translation encoded every source sentence into a fixed-length vector, regardless of its length, from which the decoder would then generate a...",
    "full_snippet": "Conventional encoder-decoder architectures for machine translation encoded every source sentence into a fixed-length vector, regardless of its length, from which the decoder would then generate a translation. This made it difficult for the neural network to cope with long sentences, essentially resulting in a performance bottleneck. The Bahdanau attention was proposed to address the performance bottleneck of conventional encoder-decoder architectures, achieving significant improvements over the conventional approach. In this tutorial, you will discover the Bahdanau attention mechanism for neural machine translation. After completing this tutorial, you will know: Kick-start your project with my book Building Transformer Models with Attention. It provides self-study tutorials with working code to guide you into building a fully-working transformer model that can\ntranslate sentences from one language to another... Let\u2019s get started. The Bahdanau attention mechanism\nPhoto by Sean Oulashin,",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "KV Caching & Attention Optimization: From O(n\u00b2) to O(n) | by pdawg | Medium",
    "url": "https://medium.com/@prathamgrover777/kv-caching-attention-optimization-from-o-n%C2%B2-to-o-n-8b605f0d4072",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We\u2019ve seen how an LLM types out a thousand-word answer, word by word, as if it\u2019s \u201cthinking out loud\u201d. It feels smooth, but behind the scenes, the process is painfully inefficient. At generation step...",
    "full_snippet": "We\u2019ve seen how an LLM types out a thousand-word answer, word by word, as if it\u2019s \u201cthinking out loud\u201d. It feels smooth, but behind the scenes, the process is painfully inefficient. At generation step t, the model must ensure the next word is consistent with everything that came before: your prompt, the partial output so far, system instructions, or any hidden context. In practical terms, the model rebuilds all previous hidden states through all its transformer layers and computes Queries, Keys, and Values again, even though the previous tokens haven\u2019t changed at all. This re-computation happens layer by layer and head by head. Nothing is reused. As shown in Fig. 1, the computation per token keeps increasing with sequence length because nothing from previous steps is reused. Why This Scales So Badly? Imagine writing a paragraph, and before you add each new sentence, you re-read the entire document from the beginning. Then you do it again for the next sentence. and again. and again. that\u2019",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Transformers are Graph Neural Networks",
    "url": "https://thegradient.pub/transformers-are-graph-neural-networks",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "My engineering friends often ask me: deep learning on graphs sounds great, but are there any real applications? While Graph Neural Networks are used in recommendation systems at Pinterest, Alibaba...",
    "full_snippet": "My engineering friends often ask me: deep learning on graphs sounds great, but are there any real applications? While Graph Neural Networks are used in recommendation systems at Pinterest, Alibaba and Twitter, a more subtle success story is the Transformer architecture, which has taken the NLP world by storm. Through this post, I want to establish a link between Graph Neural Networks (GNNs) and Transformers. I'll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we can work together to drive future progress. Let's start by talking about the purpose of model architectures\u2014representation learning. At a high level, all neural network architectures build representations of input data as vectors/embeddings, which encode useful statistical and semantic information about the data. These latent or hidden representations can then be used for performing something useful, such as classifying an image ",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Using topology for discrete problems | The Borsuk-Ulam theorem and stolen necklaces - YouTube",
    "url": "https://www.youtube.com/watch?v=yuVqxCSsE7c",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Follow along using the transcript. Follow along using the transcript.",
    "full_snippet": "Follow along using the transcript. Follow along using the transcript.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Energy-Based Models \u00b7 Deep Learning",
    "url": "https://atcold.github.io/NYU-DLSP20/en/week07/07-1",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We will introduce a new framework for defining models. It provides a unifying umbrella that helps define supervised, unsupervised and self-supervised models. Energy-based models observe a set of...",
    "full_snippet": "We will introduce a new framework for defining models. It provides a unifying umbrella that helps define supervised, unsupervised and self-supervised models. Energy-based models observe a set of variables \n\ud835\udc65\nx and output a set of variables \n\ud835\udc66\ny. There are 2 major problems with feed-forward nets: Instead of trying to classify \n\ud835\udc65\nx\u2019s to \n\ud835\udc66\ny\u2019s, we would like to predict if a certain pair of (\n\ud835\udc65\nx, \n\ud835\udc66\ny) fit together or not. Or in other words, find a \n\ud835\udc66\ny compatible with \n\ud835\udc65\nx. We can also pose the problem as finding a \n\ud835\udc66\ny for which some \n\ud835\udc39\n(\n\ud835\udc65\n,\n\ud835\udc66\n)\nF(x,y) is low. For example: We define an energy function \n\ud835\udc39\n:\n\ud835\udc4b\n\u00d7\n\ud835\udc4c\n\u2192\n\ud835\udc45\nF:X\u00d7Y\u2192R where \n\ud835\udc39\n(\n\ud835\udc65\n,\n\ud835\udc66\n)\nF(x,y) describes the level of dependency between \n(\n\ud835\udc65\n,\n\ud835\udc66\n)\n(x,y) pairs. (Note that this energy is used in inference, not in learning.) The inference is given by the following equation: We would like the energy function to be smooth and differentiable so that we can use it to perform the gradient-based method for inference. I",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "On the Biology of a Large Language Model",
    "url": "https://transformer-circuits.pub/2025/attribution-graphs/biology.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We investigate the internal mechanisms used by Claude 3.5 Haiku \u2014 Anthropic's lightweight production model \u2014 in a variety of contexts, using our circuit tracing methodology.",
    "full_snippet": "We investigate the internal mechanisms used by Claude 3.5 Haiku \u2014 Anthropic's lightweight production model \u2014 in a variety of contexts, using our circuit tracing methodology.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Galaxy brain resistance",
    "url": "https://vitalik.eth.limo/general/2025/11/07/galaxybrain.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "One important property for a style of thinking and argumentation to have is what I call galaxy brain resistance: how difficult is it to abuse that style of thinking to argue for pretty much whatever...",
    "full_snippet": "One important property for a style of thinking and argumentation to have is what I call galaxy brain resistance: how difficult is it to abuse that style of thinking to argue for pretty much whatever you want - something that you already decided elsewhere for other reasons? The spirit here is similar to falsifiability in science: if your arguments can justify anything, then your arguments imply nothing. You want to get to step 2 and then stop. It's easiest to motivate the need to think about galaxy brain resistance by looking at what happens in its absence. You've probably heard many cases of people saying things like this: We are building a new decentralized ____ marketplace that will revolutionize how ____ customers interact with their providers, and will allow creators to turn their audiences into digital nation states. $____ is a governance token that lets you play a direct role in this rapidly growing market. If we capture even 1% of the ___ market share, this will be a $___ billio",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "On Seeing Through and Unseeing: The Hacker Mindset \u00b7 Gwern.net",
    "url": "https://gwern.net/unseeing",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Defining the security/hacker mindset as extreme reductionism: ignoring the surface abstractions and limitations to treat a system as a source of parts to manipulate into a different system, with...",
    "full_snippet": "Defining the security/hacker mindset as extreme reductionism: ignoring the surface abstractions and limitations to treat a system as a source of parts to manipulate into a different system, with different (and usually unintended) capabilities.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Towards a Geometric Theory of Deep Learning - Govind Menon - YouTube",
    "url": "https://www.youtube.com/watch?v=44hfoihYfJ0",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "This lecture explores the mathematical underpinnings of deep learning, focusing on the deep linear network (DLN) model. The speaker presents sharp results on training dynamics, uncovering unexpected...",
    "full_snippet": "This lecture explores the mathematical underpinnings of deep learning, focusing on the deep linear network (DLN) model. The speaker presents sharp results on training dynamics, uncovering unexpected connections to minimal surfaces and geometric invariant theory. The analysis reveals a conceptual framework for understanding \"true\" deep learning. 1 of 78 lessons complete Follow along using the transcript. This lecture explores the mathematical underpinnings of deep learning, focusing on the deep linear network (DLN) model. The speaker presents sharp results on training dynamics, uncovering unexpected connections to minimal surfaces and geometric invariant theory. The analysis reveals a conceptual framework for understanding \"true\" deep learning. 1 of 78 lessons complete Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Reinforcement Learning algorithms summarized",
    "url": "https://letters.lossfunk.com/p/reinforcement-learning-algorithms",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "The basic task of reinforcement learning is this: given a state we\u2019re in and probabilities of different actions we can take, how do we increase or decrease those probabilities so that we increase...",
    "full_snippet": "The basic task of reinforcement learning is this: given a state we\u2019re in and probabilities of different actions we can take, how do we increase or decrease those probabilities so that we increase average rewards in future. The fundamental tradeoff in this is bias, variance (as detailed below). Thanks for reading! Subscribe for free to receive new posts and support my work. The intuitive thing to do is to increase probability of actions that are \u201cbetter\u201d (in the sense of helping a higher rewarding trajectory in future) Notice that no matter which algorithm we end up using, we need to know how \u201crewarding\u201d an action is. This reward can be calculated in two ways: Montecarlo approach: Wait for full roll-outs and sum actual rewards we get in the trajectory This is high variance, low bias because you\u2019re working with actual rewards from environment (not estimates) but it has high variance because rewards for different trajectories varies by a lot Bootstrap an estimator of rewards: learn an est",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Love Song of J. Alfred Prufrock by T. S. Eliot | Poetry Magazine",
    "url": "https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "The yellow fog that rubs its back upon the window-panes, The yellow smoke that rubs its muzzle on the window-panes, Licked its tongue into the corners of the evening, Lingered upon the pools that...",
    "full_snippet": "The yellow fog that rubs its back upon the window-panes, The yellow smoke that rubs its muzzle on the window-panes, Licked its tongue into the corners of the evening, Lingered upon the pools that stand in drains, Let fall upon its back the soot that falls from chimneys, Slipped by the terrace, made\u2026",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "thalassophilia - by hannah - a lot about nothing",
    "url": "https://substack.com/inbox/post/181977520?r=294fgl",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Lately I\u2019ve been restless as ever. I miss home and all its people and real seasons where all the leaves color the sky orange-red and dance swirling in the sweeping wind. I miss the vastness of the...",
    "full_snippet": "Lately I\u2019ve been restless as ever. I miss home and all its people and real seasons where all the leaves color the sky orange-red and dance swirling in the sweeping wind. I miss the vastness of the cornfields, so thick you could drown in it \u2014 a midwestern thalassophiliac. It\u2019s the kind of empty space that you can only fill with the thoughts of leaving (in all senses of the word). It\u2019s home, the same as it was 19 years ago and the same it will be in 19 more, just as I left it. How ironic is it that Indiana is a landlocked state? Recently, I came across the term oceanic feeling, coined by Romain Rolland in a letter to Freud. He used it to describe a sensation of \u2018eternity\u2019, an unbounded union with the universe. He attributed this incredible, metaphysical state to the source of all religious energy, the universal god. Freud, in his response, dismissed this as a deformity: a vestigial sensation left over from infancy, when the borders of the world haven\u2019t yet formed, and the edges of the eg",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Concept of the Ruliad\u2014Stephen Wolfram Writings",
    "url": "https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "I call it the ruliad. Think of it as the entangled limit of everything that is computationally possible: the result of following all possible computational rules in all possible ways. It\u2019s yet...",
    "full_snippet": "I call it the ruliad. Think of it as the entangled limit of everything that is computationally possible: the result of following all possible computational rules in all possible ways. It\u2019s yet another surprising construct that\u2019s arisen from our Physics Project. And it\u2019s one that I think has extremely deep implications\u2014both in science and beyond. In many ways, the ruliad is a strange and profoundly abstract thing. But it\u2019s something very universal\u2014a kind of ultimate limit of all abstraction and generalization. And it encapsulates not only all formal possibilities but also everything about our physical universe\u2014and everything we experience can be thought of as sampling that part of the ruliad that corresponds to our particular way of perceiving and interpreting the universe. We\u2019re going to be able to say many things about the ruliad without engaging in all its technical details. (And\u2014it should be said at the outset\u2014we\u2019re still only at the very beginning of nailing down those technical de",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Francesco Capuano",
    "url": "https://fracapuano.github.io/blog/optimizing-nns",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "TLDR: A technical blog to revisit the fundamentals of what, in the crudest sense, makes Deep Learning work. A SGD-to-Muon tour, derived from first principles in math and then implemented from scratch...",
    "full_snippet": "TLDR: A technical blog to revisit the fundamentals of what, in the crudest sense, makes Deep Learning work. A SGD-to-Muon tour, derived from first principles in math and then implemented from scratch in Jax. Take a NN parametrized with parameters \n\ud835\udf03\n\u03b8. During training, the parameters are updated using differential information relating the performance obtained to the weights used, i.e. using \n\u2207\n\ud835\udc3f\n(\n\ud835\udf03\n)\n=\n\u2211\n\ud835\udc56\n\u2208\n\ud835\udc37\n\u2207\n\u2113\n\ud835\udc56\n(\n\ud835\udf03\n)\n\u2207L(\u03b8)=\u2211\ni\u2208D\n\t\u200b\n\n\u2207\u2113\ni\n\t\u200b\n\n(\u03b8), so that weights are iteratively updated according to: where \n\ud835\udc53\nf is some function of the weights \n\ud835\udf03\n\ud835\udc61\n\u2212\n1\n\u03b8\nt\u22121\n\t\u200b\n\n and gradients \n\u2207\n\ud835\udc3f\n(\n\ud835\udf03\n\ud835\udc61\n\u2212\n1\n)\n\u2207L(\u03b8\nt\u22121\n\t\u200b\n\n). For both conceptual and computational reason, one typically does not use the exact gradient of the loss \n\u2207\n\ud835\udc3f\n(\n\ud835\udf03\n)\n\u2207L(\u03b8), and rather relies on \n1\n\u2223\n\ud835\udc35\n\u2223\n\u2211\n\ud835\udc56\n\u2208\n\ud835\udc35\n\u2207\n\u2113\n\ud835\udc56\n(\n\ud835\udf03\n)\n\u2223B\u2223\n1\n\t\u200b\n\n\u2211\ni\u2208B\n\t\u200b\n\n\u2207\u2113\ni\n\t\u200b\n\n(\u03b8), referred to as the stochastic gradient for the mini-batch \n\ud835\udc35\n\u2282\n\ud835\udc37\n:\n\ud835\udc35\n\u223c\n\ud835\udc37\nB\u2282D:B\u223cD. On a conceptual level, stochastic gradients suffer less from poor i",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Just quit - Arjun Raj",
    "url": "https://arjunrajlab.substack.com/p/just-quit",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We spend a lot of time as scientists thinking about how to choose a project\u2014and that is, of course, critically important to success. But\u2026 no matter how carefully you try to pick out the most...",
    "full_snippet": "We spend a lot of time as scientists thinking about how to choose a project\u2014and that is, of course, critically important to success. But\u2026 no matter how carefully you try to pick out the most groundbreaking, innovative project imaginable, the simple truth is that not every project is going to be awesome. Consequently, just as important as the skill of choosing a project is the skill of knowing when to quit a project. In my view, we should quit far more often than we do, for the simple reason that time is so very precious. Here is possibly the scariest diagram of all time: That is not a lot of weeks. Each scientific project can take up 2-4 ENTIRE COLUMNS. As mentioned, the success of a project is way more probabilistic than we care to admit. So you have to sample, and that means rejecting many samples. Do not let this precious time go to waste. Sometimes, you just have to quit. Why is it so hard to quit in science? Here are a few top reasons, all of which are based on fear: Fear of the u",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  }
]