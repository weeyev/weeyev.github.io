[
  {
    "title": "How the Brain and AI Reuse Old Knowledge in New Situations - Kempner Institute",
    "url": "https://kempnerinstitute.harvard.edu/news/how-the-brain-and-ai-reuse-old-knowledge-in-new-situations",
    "date": "February 27, 2026",
    "domain": "",
    "snippet": "Humans and other animals are remarkably good at using old knowledge in new situations. This ability \u2014 known as generalization \u2014 allows us to recognize a friend in an unexpected [\u2026]",
    "full_snippet": "Humans and other animals are remarkably good at using old knowledge in new situations. This ability \u2014 known as generalization \u2014 allows us to recognize a friend in an unexpected [\u2026]",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Why Diffusion Language Models Are the Future | Dimitri von R\u00fctte",
    "url": "https://dimitri.ml/posts/why-diffusion-language-models-are-the-future",
    "date": "February 27, 2026",
    "domain": "",
    "snippet": "Lessons learned from working with discrete diffusion language models, and more or less speculative predictions about their future.",
    "full_snippet": "Lessons learned from working with discrete diffusion language models, and more or less speculative predictions about their future.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Spiritual practices strongly associated with reduced risk for hazardous alcohol and drug use",
    "url": "https://hsph.harvard.edu/news/spiritual-practices-strongly-associated-with-reduced-risk-for-hazardous-alcohol-and-drug-use",
    "date": "February 27, 2026",
    "domain": "",
    "snippet": "Spirituality\u2014religious or otherwise\u2014may be protective against substance misuse, according to a new Harvard Chan School study.",
    "full_snippet": "Spirituality\u2014religious or otherwise\u2014may be protective against substance misuse, according to a new Harvard Chan School study.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Exploring the Asymmetry of Life ",
    "url": "https://gsas.harvard.edu/news/exploring-asymmetry-life",
    "date": "February 27, 2026",
    "domain": "",
    "snippet": "Recent alumnus S. Furkan Ozturk, PhD \u201924, talks about a life-changing summer camp experience, going to college during a coup attempt and ISIS bombings, and searching for the origins of life through...",
    "full_snippet": "Recent alumnus S. Furkan Ozturk, PhD \u201924, talks about a life-changing summer camp experience, going to college during a coup attempt and ISIS bombings, and searching for the origins of life through the physics of molecular symmetry-breaking.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Deriving the KL divergence loss in variational autoencoders",
    "url": "https://kvfrans.com/deriving-the-kl",
    "date": "February 26, 2026",
    "domain": "",
    "snippet": "Let's derive some things related to variational auto-encoders (VAEs).  Evidence Lower Bound (ELBO) First, we'll state some assumptions. We have a dataset of images, xxx. We'll assume that each image...",
    "full_snippet": "Let's derive some things related to variational auto-encoders (VAEs).  Evidence Lower Bound (ELBO) First, we'll state some assumptions. We have a dataset of images, xxx. We'll assume that each image is generated from some unseen latent code zzz, and there's an underlying distribution of latents p(zp(zp(z). We'd",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Taste for Makers",
    "url": "https://paulgraham.com/taste.html",
    "date": "February 26, 2026",
    "domain": "",
    "snippet": "\t\t\n\n\n\n\n\n\nFebruary 2002\n\n\n\"...Copernicus' aesthetic objections to [equants] provided one essential motive for his rejection of the Ptolemaic system....\"\n\n- Thomas Kuhn, The Copernican Revolution\n\n\"All...",
    "full_snippet": "\t\t\n\n\n\n\n\n\nFebruary 2002\n\n\n\"...Copernicus' aesthetic objections to [equants] provided one essential motive for his rejection of the Ptolemaic system....\"\n\n- Thomas Kuhn, The Copernican Revolution\n\n\"All of us had been trained by Kelly Johnson and believed fanatically in his insistence that an airplane that looked beautiful would fly the same way.\"\n\n- Ben Rich, Skunk Works\n\n\"Beauty is the first test: there is no permanent place in this world for ugly mathematics.\"\n\n- G. H. Hardy, A Mathematician's Apology\n\n\nI was talking recently to a friend who teaches at MIT. His field is hot now and every year he is inundated by applications from would-be graduate students. \"A lot of them seem smart,\" he said. \"What I can't tell is whether they have any kind of taste.\"\n\nTaste. You don't hear that word much now. And yet we still need the underlying concept, whatever we call it. What my friend meant was that he wanted students who were not just good technicians, but who could use their technical knowledge",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "On Stress \u00b7 Gwern.net",
    "url": "https://gwern.net/stress",
    "date": "February 26, 2026",
    "domain": "",
    "snippet": "Stoic meditation with reference to being homeless. Written to myself at a particularly low point; like many, I take comfort in considering how things could be worse.",
    "full_snippet": "Stoic meditation with reference to being homeless. Written to myself at a particularly low point; like many, I take comfort in considering how things could be worse.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "P2P No. 20 \u2014 The Gladius of Comparison",
    "url": "https://path2phd.substack.com/p/p2p-no-20-the-gladius-of-comparison?s=r",
    "date": "February 26, 2026",
    "domain": "",
    "snippet": "Compare forward, but not backward.",
    "full_snippet": "Compare forward, but not backward.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The ReLU illusion of progress - by Patrik Reizinger",
    "url": "https://path2phd.substack.com/p/the-relu-illusion-of-progress?s=r",
    "date": "February 26, 2026",
    "domain": "",
    "snippet": "A message to first-year PhD students who feel like they have nothing to show.",
    "full_snippet": "A message to first-year PhD students who feel like they have nothing to show.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Surprisingly Powerful Influence of Drawing on Memory",
    "url": "https://fermatslibrary.com/s/the-surprisingly-powerful-influence-of-drawing-on-memory",
    "date": "February 25, 2026",
    "domain": "",
    "snippet": "Fermat&#39;s Library is a platform for illuminating academic papers.",
    "full_snippet": "Fermat&#39;s Library is a platform for illuminating academic papers.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "[2405.07987] The Platonic Representation Hypothesis",
    "url": "https://arxiv.org/pdf/2405.07987.pdf",
    "date": "February 25, 2026",
    "domain": "",
    "snippet": "Abstract:We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple...",
    "full_snippet": "Abstract:We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Making a Tiny Mac From a Raspberry Pi Zero",
    "url": "https://www.instructables.com/Making-a-Tiny-Mac-From-a-Raspberry-Pi-Zero",
    "date": "February 22, 2026",
    "domain": "",
    "snippet": "Making a Tiny Mac From a Raspberry Pi Zero: Years ago I saw that John Leake built a 1/3 scale Macintosh. He made his before cheap 3d printers were everywhere. His was made from scratch from sheets of...",
    "full_snippet": "Making a Tiny Mac From a Raspberry Pi Zero: Years ago I saw that John Leake built a 1/3 scale Macintosh. He made his before cheap 3d printers were everywhere. His was made from scratch from sheets of PVC, sanded and painted. He needed custom cables as well as solder work on the Pi. I was so e\u2026",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Shape Rotation 101: An Intro to Einsum and Jax Transformers ",
    "url": "https://sankalp.bearblog.dev/einsum-new",
    "date": "February 21, 2026",
    "domain": "",
    "snippet": "Acknowledgements\r\n\r\nFirst, I would like to acknowledge my friends and kind internet strangers who helped me with this post.\r\n\r\nThis post heavily adapts from ...",
    "full_snippet": "Acknowledgements\r\n\r\nFirst, I would like to acknowledge my friends and kind internet strangers who helped me with this post.\r\n\r\nThis post heavily adapts from ...",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Which Future?",
    "url": "https://michaelnotebook.com/whichfuture",
    "date": "February 21, 2026",
    "domain": "",
    "snippet": "This essay is the text for a talk on how to wisely navigate risks from transformative technology, especially artificial superintelligence (ASI). It was given at Astera on January 28, 2026. In 1954...",
    "full_snippet": "This essay is the text for a talk on how to wisely navigate risks from transformative technology, especially artificial superintelligence (ASI). It was given at Astera on January 28, 2026. In 1954 the United States carried out its first full-scale test of a thermonuclear bomb, on Bikini Atoll in the Western Pacific. Known as Castle Bravo, the bomb's designers expected a 6 megaton blast. They were shocked when it yielded 15 megatons, an excess of 9 megatons, about 600 times the Hiroshima blast. The unexpected radiation fallout caused many deaths \u2013 the exact number is disputed \u2013 and serious radiation exposure to more than a thousand people, triggering a major international incident. What went wrong? The bomb contained both the lithium-6 and lithium-7 isotopes of lithium. The bomb's designers believed only the lithium-6 would contribute to the yield, while the lithium-7 would be inert. But the lithium-7 converted to tritium far faster than expected, and that turned out to almost triple th",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "frontier model training methodologies | Alex Wa\u2019s Blog",
    "url": "https://djdumpling.github.io/2026/01/31/frontier_training.html",
    "date": "February 20, 2026",
    "domain": "",
    "snippet": "How do labs train a frontier, multi-billion parameter model? We look towards seven open-weight frontier models: Hugging Face\u2019s SmolLM3, Prime Intellect\u2019s Intellect 3, Nous Research\u2019s Hermes 4,...",
    "full_snippet": "How do labs train a frontier, multi-billion parameter model? We look towards seven open-weight frontier models: Hugging Face\u2019s SmolLM3, Prime Intellect\u2019s Intellect 3, Nous Research\u2019s Hermes 4, OpenAI\u2019s gpt-oss-120b, Moonshot\u2019s Kimi K2, DeepSeek\u2019s DeepSeek-R1, and Arcee\u2019s Trinity series. This blog is an attempt at distilling the techniques, motivations, and considerations used to train their models with an emphasis on training methodology over infrastructure.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Why I Write - Niko McCarty",
    "url": "https://nikomc.com/2025/01/01/why-i-write",
    "date": "February 20, 2026",
    "domain": "",
    "snippet": "In the summer of 1946, shortly after the close of World War II, George Orwell published a short essay entitled \u201cWhy I Write.\u201d He had already released Coming Up for Air, Keep the Aspidistra Flying (my...",
    "full_snippet": "In the summer of 1946, shortly after the close of World War II, George Orwell published a short essay entitled \u201cWhy I Write.\u201d He had already released Coming Up for Air, Keep the Aspidistra Flying (my favorite Orwell novel), and Animal Farm \u2014 the last of which, published in 1945, launched Orwell to immense fame for the first time in his life. His essay on writing didn\u2019t reach nearly as large an audience as his books, but it landed at a decisive moment, when people were deeply skeptical of propaganda and the lines between facts and political agendas had blurred in the aftermath of global conflict.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "NaturalProofs: Mathematical Theorem Proving in Natural Language",
    "url": "https://arxiv.org/pdf/2104.01112.pdf",
    "date": "February 20, 2026",
    "domain": "",
    "snippet": "Abstract:Understanding and creating mathematics using natural mathematical language - the mixture of symbolic and natural language used by humans - is a challenging and important problem for driving...",
    "full_snippet": "Abstract:Understanding and creating mathematics using natural mathematical language - the mixture of symbolic and natural language used by humans - is a challenging and important problem for driving progress in machine learning. As a step in this direction, we develop NaturalProofs, a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NaturalProofs unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization. Using NaturalProofs, we benchmark strong neural methods on mathematical reference retrieval and generation tasks which test a system's ability to determine key results that appear in a proof. Large-scale sequence models show promise compared to classical information retrieval methods, yet their performance and out-of-domain generalization leave substantial room for improvement. NaturalProofs opens many avenues for research on challenging mathematical tasks.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Your Transformer is Secretly an EOT Solver | Elements of a Vector Space",
    "url": "https://elonlit.com/scrivings/your-transformer-is-secretly-an-eot-solver",
    "date": "February 20, 2026",
    "domain": "",
    "snippet": "How the search for a simple approximation revealed a beautiful, exact solution.",
    "full_snippet": "How the search for a simple approximation revealed a beautiful, exact solution.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Are there any mathematical knots that exist in dimensions higher than 3? ",
    "url": "https://www.quora.com/Are-there-any-mathematical-knots-that-exist-in-dimensions-higher-than-3-If-no-such-knots-exist-are-there-any-analogs-that-are-similar-but-follow-a-different-definition",
    "date": "February 14, 2026",
    "domain": "",
    "snippet": "There are no nontrivial knots that live in four- or higher-dimensional spaces, because if you have four dimensions to work in you can easily untie any knot. Of course a knot is just an embedding of a...",
    "full_snippet": "There are no nontrivial knots that live in four- or higher-dimensional spaces, because if you have four dimensions to work in you can easily untie any knot. Of course a knot is just an embedding of a circle into a space up to (ambient) isotopy. If you wanted a higher-dimensional analogue, you could replace a circle with a higher-dimensional analogue. The obvious analogue of a circle is a sphere or hypersphere, so you could consider embeddings of an n-sphere up to isotopy. Or you could realize that the circle is the only compact 1-manifold, and consider embeddings of a fixed compact manifold (say, a torus) up to isotopy. The most general problem is just to fix two manifolds and consider isotopy classes of embeddings of the first into the second. Knot theory is just the case of this problem where we consider embeddings of \nS\n1\n\ud835\udc46\n1\n into \nR\n3\n\ud835\udc45\n3\n. In fact, Zeeman writes that there are essentially three classical problems in topology: \nThe homeomorphism problem has been solved for one- ",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Richard Price, Bayes' theorem, and God",
    "url": "https://www.york.ac.uk/depts/maths/histstat/price.pdf",
    "date": "February 14, 2026",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Conscious AI? Not Even Close! | Brain Inspired",
    "url": "https://braininspired.co/podcast/231",
    "date": "February 13, 2026",
    "domain": "",
    "snippet": "The Transmitter is an online publication that aims to deliver useful information, insights and tools to build bridges across neuroscience and advance research. Visit thetransmitter.org to explore the...",
    "full_snippet": "The Transmitter is an online publication that aims to deliver useful information, insights and tools to build bridges across neuroscience and advance research. Visit thetransmitter.org to explore the latest neuroscience news and perspectives, written by journalists and scientists. Read more about our partnership. Sign up for Brain Inspired email alerts to be notified every time a new Brain Inspired episode is released. To explore more neuroscience news and perspectives, visit thetransmitter.org. Jaan Aru is a co-principal investigator of the Natural and Artificial Intelligence Lab at the University of Tartu in Estonia, where he is an associate professor. Jaan\u2019s name has kept popping up on papers I\u2019ve read over the last few years, sometimes alongside other guests I\u2019ve had on the podcast, like Matthew Larkum and Mac Shine. With those people and others, he has co-authored papers exploring how some of the pesky biological details of brains might be important for our subjective conscious ex",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Singularity will Occur on a Tuesday - Cam Pedersen",
    "url": "https://campedersen.com/singularity",
    "date": "February 11, 2026",
    "domain": "",
    "snippet": "src={alwaysHasBeen.src} alt=\"Always has been astronaut meme\" / \"Wait, the singularity is just humans freaking out?\" \"Always has been.\" Everyone in <span style={",
    "full_snippet": "src={alwaysHasBeen.src} alt=\"Always has been astronaut meme\" / \"Wait, the singularity is just humans freaking out?\" \"Always has been.\" Everyone in <span style={",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Smallest Eigenvalues of a Graph Laplacian",
    "url": "http://blog.shriphani.com/2015/04/06/the-smallest-eigenvalues-of-a-graph-laplacian",
    "date": "February 9, 2026",
    "domain": "",
    "snippet": "Given a graph $ G = (V, E) $, its adjacency matrix $ A $ contains an entry at $ A_{ij} $ if vertices $ i $ and $ j $ have an edge between them. The degree matrix $ D $ contains the degree of each...",
    "full_snippet": "Given a graph $ G = (V, E) $, its adjacency matrix $ A $ contains an entry at $ A_{ij} $ if vertices $ i $ and $ j $ have an edge between them. The degree matrix $ D $ contains the degree of each vertex along its diagonal.  The graph laplacian of $ G $ is...",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "A Short Tutorial on Graph Laplacians, Laplacian Embedding, and Spectral Clustering",
    "url": "https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf",
    "date": "February 9, 2026",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": " Metric Learning and Manifolds: Preserving the Intrinsic Geometry - YouTube",
    "url": "https://www.youtube.com/watch?v=kTJoFLcdtn8",
    "date": "February 9, 2026",
    "domain": "",
    "snippet": "Follow along using the transcript. Follow along using the transcript.",
    "full_snippet": "Follow along using the transcript. Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Differential geometry of ML",
    "url": "https://research.fal.ai/blog/differential-geometry-of-ml",
    "date": "February 9, 2026",
    "domain": "",
    "snippet": "Machine learning has achieved remarkable advancements largely due to the success of gradient descent algorithms. To gain deeper mathematical insight into these algorithms, it is essential to adopt an...",
    "full_snippet": "Machine learning has achieved remarkable advancements largely due to the success of gradient descent algorithms. To gain deeper mathematical insight into these algorithms, it is essential to adopt an accurate geometric perspective. In this article, we introduce the fundamental notion of a manifold as a mathematical abstraction of continuous spaces. By providing a clear geometric interpretation of gradient descent within this manifold framework, we aim to help readers develop a precise understanding of gradient descent algorithms.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Togelius: Math and me",
    "url": "https://togelius.blogspot.com/2026/02/math-and-me.html",
    "date": "February 9, 2026",
    "domain": "",
    "snippet": "For most of my adult life, I was too cowardly to write this text, never mind posting it. I was worried about what people would think, and th...",
    "full_snippet": "For most of my adult life, I was too cowardly to write this text, never mind posting it. I was worried about what people would think, and th...",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Fibre integrals and the Thom isomorphism",
    "url": "https://lucaman99.github.io/mathblog/fibre_integrals.html",
    "date": "February 8, 2026",
    "domain": "",
    "snippet": "Today, I want to briefly go over the formal definition of a fibre integral, a very useful construction which is sometimes invoked a bit flippantly in some differential topology proofs. The idea is...",
    "full_snippet": "Today, I want to briefly go over the formal definition of a fibre integral, a very useful construction which is sometimes invoked a bit flippantly in some differential topology proofs. The idea is that, given a differential form on the entire space of a fibre bundle, one should be able to \"integrate out\" the vertical part of the form along the fibres, yielding a form on the base space. Note: This blog post was updated on September 8th, 2025 to fix a few issues and add (quite a bit) of additional information. The first order of business will be proving a generalization of the regular value submanifold theorem which apply to manifolds with boundary, as we will be defining fibre integrals for manifolds with and without boundary. Lemma. If \n\ud835\udc40\n is a smooth manifold without boundary and \n\ud835\udc53\n:\n\ud835\udc40\n\u2192\n\ud835\udc45\n is smooth, with regular values \n\ud835\udc4e\n and \n\ud835\udc4f\n, then \n\ud835\udc46\n=\n\ud835\udc53\n\u2212\n1\n(\n[\n\ud835\udc4e\n,\n\ud835\udc4f\n]\n)\n is an embedded submanifold with boundary of \n\ud835\udc40\n of equal dimension to \n\ud835\udc40\n. Moreover, the boundary of \n\ud835\udc46\n is ",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "A Decade of Residuals: History & Effects on modern ML",
    "url": "https://dhia-naouali.github.io/blogs_notes/a-decade-of-residuals",
    "date": "February 7, 2026",
    "domain": "",
    "snippet": "Skip to content\nA Decade of Residuals: History & Effects on modern ML\nIntroduction: Gradient Highways\nA decade ago training deep neural nets was quite the bottleneck in ML. Increasing depth,...",
    "full_snippet": "Skip to content\nA Decade of Residuals: History & Effects on modern ML\nIntroduction: Gradient Highways\nA decade ago training deep neural nets was quite the bottleneck in ML. Increasing depth, counter-intuitively, reduced performance. although we knew that depth meant better representation in theory, but in practice training was brittle and numerically unstable as the chain of jacobians gets longer and longer, many work-arounds existed such as layer-wise pre-training, semi-supervised stacking, better initializations \u2026\nuntil a different approach succesfully mitigated a major design flaw in pure sequential neural nets: the vanishing gradient problem, espeically pronounced when sigmoid and tanh a common activation functions choice at that time.\nResidual connections were introduced to solve that problem but over time this solution influenced a wide range of design choices, training strategies and representation learning dynamics.\nnote: figures in this blog reflect a focus on the residual str",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Godel, Escher, Bach- An Eternal Golden Braid",
    "url": "https://www.physixfan.com/wp-content/files/GEBen.pdf",
    "date": "February 6, 2026",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": " Learning world model learning from scratch",
    "url": "https://github.com/pham-tuan-binh/learning-world-model-learning/tree/main",
    "date": "February 6, 2026",
    "domain": "",
    "snippet": "Learning world model learning from scratch. Contribute to pham-tuan-binh/learning-world-model-learning development by creating an account on GitHub.",
    "full_snippet": "Learning world model learning from scratch. Contribute to pham-tuan-binh/learning-world-model-learning development by creating an account on GitHub.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "AlmondGod/tinyworlds: A minimal implementation of DeepMind's Genie world model",
    "url": "https://github.com/AlmondGod/tinyworlds?tab=readme-ov-file",
    "date": "February 6, 2026",
    "domain": "",
    "snippet": "A minimal implementation of DeepMind's Genie world model",
    "full_snippet": "A minimal implementation of DeepMind's Genie world model",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "ongoing survey on starcraft research",
    "url": "https://yobibyte.github.io/starcraft_research.html",
    "date": "February 5, 2026",
    "domain": "",
    "snippet": "I have wanted to get into StarCraft research for a long time. Actually, right after I played StarCraft II for the first time. (I did not care about research when I tried the BroodWar at school =( )....",
    "full_snippet": "I have wanted to get into StarCraft research for a long time. Actually, right after I played StarCraft II for the first time. (I did not care about research when I tried the BroodWar at school =( ). Though, I have never done anything related apart from reading some papers on the topic. Now I feel like it\u2019s the right time to start. What is the first step? Reading up! To make it more useful for myself and for the community, I\u2019d like to summarize all the papers on the topic I read and make a kind of overview of what have been done already. I\u2019ve read amazing surveys [1,2] on SCI research and consider them to be one of the best resources on the topic. However it\u2019s kinda outdated and I would like to pick up the baton. I was thinking about writing a paper with the overview, but then decided to make it a blogpost. It\u2019s much easier to update and make it an ongoing overview. It\u2019s nice to have a possibility to add a new paper to the review several days after it\u2019s published. I will start from Star",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "I miss thinking hard.",
    "url": "https://www.jernesto.com/articles/thinking_hard",
    "date": "February 5, 2026",
    "domain": "",
    "snippet": "By \u201cthinking hard,\u201d I mean encountering a specific, difficult problem and spending multiple days just sitting with it to overcome it. a) All the time. b) Never. c) Somewhere in between. If your...",
    "full_snippet": "By \u201cthinking hard,\u201d I mean encountering a specific, difficult problem and spending multiple days just sitting with it to overcome it. a) All the time. b) Never. c) Somewhere in between. If your answer is (a) or (b), this post isn't for you. But if, like me, your response is (c), you might get something out of this, if only the feeling that you aren't alone. First, a disclaimer: this post has no answers, not even suggestions. It is simply a way to vent something I've been feeling for the last few months. I believe my personality is built on two primary traits: The Builder (The desire to create, ship, and be pragmatic). The Thinker (The need for deep, prolonged mental struggle). The builder is pretty self explanatory, it\u2019s motivated by velocity and utility. It is the part of me that craves the transition from \u201cidea\u201d to \u201creality.\u201d It loves the dopamine hit of a successful deploy, the satisfaction of building systems to solve real problems, and the knowledge that someone, somewhere, is usi",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Advice for research projects",
    "url": "https://rockt.ai/2018/08/29/msc-advice",
    "date": "February 5, 2026",
    "domain": "",
    "snippet": "Every year we get contacted by students who wish to work on short-term machine learning research projects with us. By now, we have supervised a good number of them and we noticed that some of the...",
    "full_snippet": "Every year we get contacted by students who wish to work on short-term machine learning research projects with us. By now, we have supervised a good number of them and we noticed that some of the advice that we gave followed a few recurring principles. In this post, we share what we believe is good advice for a master\u2019s thesis project or a summer research internship in machine learning. This post is by no means comprehensive but instead emphasizes those pitfalls that we saw over and over again. For instance, we will not talk about how to pick a good project or how to generally approach a machine learning research project. Some of our advice is generally applicable for working on machine learning and specifically deep and/or reinforcement research projects. However, some of it is only important when faced with the time constraints of a three-month project and are considerably less important when you just started the journey of a three to five year Ph.D. degree. Machine learning and spec",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Discrete Calculus | Ji-Ha's Blog",
    "url": "https://jiha-kim.github.io/posts/discrete-calculus",
    "date": "February 5, 2026",
    "domain": "",
    "snippet": "An introduction to Discrete Calculus, a theory for sums and differences of sequences as opposed to derivatives and integrals of functions in infinitesimal calculus.",
    "full_snippet": "An introduction to Discrete Calculus, a theory for sums and differences of sequences as opposed to derivatives and integrals of functions in infinitesimal calculus.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Splintered Mind: Is Signal Strength a Confound in Consciousness Research?",
    "url": "https://schwitzsplinters.blogspot.com/2026/01/is-signal-strength-confound-in.html?m=1",
    "date": "February 5, 2026",
    "domain": "",
    "snippet": "But Michel does make one claim that bugs me, and that claim is central to the article. And Hakwan Lau -- another otherwise terrific methodologist -- makes a similar claim in his 2022 book In...",
    "full_snippet": "But Michel does make one claim that bugs me, and that claim is central to the article. And Hakwan Lau -- another otherwise terrific methodologist -- makes a similar claim in his 2022 book In Consciousness We Trust, and again the claim is central to the argument of that book. So today I'm going to poke at that claim, and maybe it will burst like a sour blueberry. The claim: Signal strength (performance capacity, in Lau's version) is a confound in consciousness research. As Michel uses the phrase, \"signal strength\" is how discriminable a perceptible feature is to a subject. A sudden, loud blast of noise has high signal strength. It's very easy to notice. A faint wavy pattern in a gray field, presented for a tenth of second, has low signal strength. It is easy to miss. Importantly, signal strength is not the same as (objective, externally measurable) stimulus intensity, but reflects how well the perceiver responds to the signal. Signal strength clearly correlates with consciousness. You'r",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Post 38: On Slack - Having room to be excited \u2014 Neel Nanda",
    "url": "https://www.neelnanda.io/blog/38-slack",
    "date": "February 5, 2026",
    "domain": "",
    "snippet": "On the importance of Slack - the freedom and spare capacity left on your life. How to guard and protect your Slack, notice the bottlenecks which bleed away your Slack, notice the drive to optimise...",
    "full_snippet": "On the importance of Slack - the freedom and spare capacity left on your life. How to guard and protect your Slack, notice the bottlenecks which bleed away your Slack, notice the drive to optimise that pushes you beyond your limits, and how to channel these insights having the freedom to be excited,",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "writing RSS reader in 80 lines of bash",
    "url": "https://yobibyte.github.io/yr",
    "date": "February 4, 2026",
    "domain": "",
    "snippet": "I consume most of my information through RSS. RSS is amazing. Up until yesterday, I used Newsboat to go through my feed, and had a ,a macro that appended a link from the item to a text file with...",
    "full_snippet": "I consume most of my information through RSS. RSS is amazing. Up until yesterday, I used Newsboat to go through my feed, and had a ,a macro that appended a link from the item to a text file with links. Every morning, I would go through my whole feed, and get a bunch of links appended to my links.md file. After that, I start reading them by copying one line at a time, and running the script that dumps the content of the link via w3m and opens a vim buffer with it. If it is a large read, I would save it as a txt file and send it to my e-reader. Recently, I started reducing my dependencies on external software. Yesterday, I thought, why not write an RSS reader myself. It will not be as powerful and robust as my current one, but it will be simpler, more hackable, and mine, i.e. I will not be dependent on a stranger's commit that might break my system accidentally or on purpose. First, I wanted to write it in Zig, as this is the language I have been recently playing with and want to get bet",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Dario Amodei \u2014\u00a0The Adolescence of Technology",
    "url": "https://www.darioamodei.com/essay/the-adolescence-of-technology",
    "date": "January 29, 2026",
    "domain": "",
    "snippet": "Confronting and Overcoming the Risks of Powerful AI",
    "full_snippet": "Confronting and Overcoming the Risks of Powerful AI",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Eleven years in AI: What does it actually mean to be a researcher?",
    "url": "https://loud-phalange-7f5.notion.site/Eleven-years-in-AI-What-does-it-actually-mean-to-be-a-researcher-2d56d9bccef780038ae9c27ffab59404",
    "date": "January 26, 2026",
    "domain": "",
    "snippet": "A tool that connects everyday work into one space. It gives you and your teams AI tools\u2014search, writing, note-taking\u2014inside an all-in-one, flexible workspace.",
    "full_snippet": "A tool that connects everyday work into one space. It gives you and your teams AI tools\u2014search, writing, note-taking\u2014inside an all-in-one, flexible workspace.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Life on Claude Nine - Igor Babuschkin",
    "url": "https://babuschk.in/posts/2026-01-25-life-on-claude-nine.html",
    "date": "January 25, 2026",
    "domain": "",
    "snippet": "It started with automating his emails.",
    "full_snippet": "It started with automating his emails.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Five ways to be stupid- Le Cun",
    "url": "https://docs.google.com/document/d/1lz8PaTIXrfRsQtbWE0ta_qrpjZi6GUAErwJmmkBay2Y/edit?tab=t.0",
    "date": "January 24, 2026",
    "domain": "",
    "snippet": "Five Ways to Act Deluded,  Stupid, Ineffective, or Evil Yann LeCun 2025-04-28 [semi-humorous, geeky political satire ahead]  Introduction Cognitive Science has proposed various models of how humans...",
    "full_snippet": "Five Ways to Act Deluded,  Stupid, Ineffective, or Evil Yann LeCun 2025-04-28 [semi-humorous, geeky political satire ahead]  Introduction Cognitive Science has proposed various models of how humans and animals perceive the world, reason about a situation, plan actions to accomplish tasks, and mak...",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Giving University Exams in the Age of Chatbots",
    "url": "https://ploum.net/2026-01-19-exam-with-chatbots.html",
    "date": "January 22, 2026",
    "domain": "",
    "snippet": "Giving University Exams in the Age of Chatbots par Ploum - Lionel Dricot.",
    "full_snippet": "Giving University Exams in the Age of Chatbots par Ploum - Lionel Dricot.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "All 325+ Competing Consciousness Theories In One Video",
    "url": "https://www.youtube.com/watch?v=h5G6Oc_V3Lw",
    "date": "January 21, 2026",
    "domain": "",
    "snippet": "Follow along using the transcript. Follow along using the transcript.",
    "full_snippet": "Follow along using the transcript. Follow along using the transcript.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "An Unofficial Guide to Prepare for a Research Position Application",
    "url": "https://pub.sakana.ai/Unofficial_Guide",
    "date": "January 20, 2026",
    "domain": "",
    "snippet": "A candid look at what we look for when interviewing research candidates at Sakana AI. The core principle? Understanding over implementation.",
    "full_snippet": "A candid look at what we look for when interviewing research candidates at Sakana AI. The core principle? Understanding over implementation.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Berggruen Institute",
    "url": "https://berggruen.org/eu/news/the-mind-as-a-city-a-systems-view-of-consciousness-by-ian-reppel",
    "date": "January 18, 2026",
    "domain": "",
    "snippet": "Ideas for a Changing World.",
    "full_snippet": "Ideas for a Changing World.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Illicit Love Letters: Albert Camus and Maria Casares",
    "url": "https://www.theparisreview.org/blog/2018/04/11/illicit-love-letters-albert-camus-and-maria-casares",
    "date": "January 17, 2026",
    "domain": "",
    "snippet": "For the past few weeks, I\u2019ve fixated on a collection of primary source material that reads like a tidy work of epistolary fiction. It\u2019s a big book, nearly 1,300 pages, transcribed from original...",
    "full_snippet": "For the past few weeks, I\u2019ve fixated on a collection of primary source material that reads like a tidy work of epistolary fiction. It\u2019s a big book, nearly 1,300 pages, transcribed from original letters, postcards, and telegrams sent between the French philosopher and writer Albert\u00a0Camus\u00a0and the Spanish French actress Maria Casares between 1944 and 1959. It\u2019s too heavy a book to bring on the subway, so I downloaded the electronic version on my phone. My camera roll is now nearly a hundred screenshots of exchanges in French between the two lovers. The book was published in France by Gallimard and has not yet been translated into English. The romance of\u00a0Camus\u00a0and Casares is richer, if not sadder, when considered alongside the narratives of each of their work. There is an eerie doubling of life and art. Absurdity is the only certainty, and this is confirmed over and over again by coincidence and chance. The two first met on June 6, 1944, the storied day the Allied forces landed in Normandy",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "\u2018He was in mystic delirium\u2019: was this hermit mathematician a forgotten genius whose ideas could transform AI \u2013 or a lonely madman? | Mathematics | The Guardian",
    "url": "https://www.theguardian.com/science/article/2024/aug/31/alexander-grothendieck-huawei-ai-artificial-intelligence",
    "date": "January 16, 2026",
    "domain": "",
    "snippet": "In isolation, Alexander Grothendieck seemed to have lost touch with reality, but some say his metaphysical theories could contain wonders",
    "full_snippet": "In isolation, Alexander Grothendieck seemed to have lost touch with reality, but some say his metaphysical theories could contain wonders",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Mythology Of Conscious AI",
    "url": "https://www.noemamag.com/the-mythology-of-conscious-ai",
    "date": "January 15, 2026",
    "domain": "",
    "snippet": "Why consciousness is more likely a property of life than of computation and why creating conscious, or even conscious-seeming AI, is a bad idea.",
    "full_snippet": "Why consciousness is more likely a property of life than of computation and why creating conscious, or even conscious-seeming AI, is a bad idea.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Where physics and biology meet - ScienceDirect",
    "url": "https://www.sciencedirect.com/science/article/pii/S0960982224011345?via%3Dihub",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "All content on this site: Copyright \u00a9 2026 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies....",
    "full_snippet": "All content on this site: Copyright \u00a9 2026 Elsevier B.V., its licensors, and contributors. All rights are reserved, including those for text and data mining, AI training, and similar technologies. For all open access content, the relevant licensing terms apply. These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly. These cookies allow us to count visits and traffic sources so we can mea",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Colors Of Her Coat - by Scott Alexander",
    "url": "https://www.astralcodexten.com/p/the-colors-of-her-coat",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "...",
    "full_snippet": "...",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Matthew Walker's \"Why We Sleep\" Is Riddled with Scientific and Factual Errors - Alexey Guzey",
    "url": "https://guzey.com/books/why-we-sleep",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "See discussion of this essay on the forum, Hacker News (a), Marginal Revolution (a), Andrew Gelman\u2019s blog 1 (a), 2 (a), 3 (a), 4 (a), /r/slatestarcodex (a), Twitter (a), listen to BBC interviewing me...",
    "full_snippet": "See discussion of this essay on the forum, Hacker News (a), Marginal Revolution (a), Andrew Gelman\u2019s blog 1 (a), 2 (a), 3 (a), 4 (a), /r/slatestarcodex (a), Twitter (a), listen to BBC interviewing me and Walker himself about it or listen to my interview with Smart People Podcast discussing it.\n\nNote: I link to a bunch of paywalled studies in this essay. Please do not use sci-hub to access them for free and do not use this trick (a) to easily redirect papers to sci-hub.\n\nFor the clearest \u2026",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Post 51: Socratic Persuasion: Giving Opinionated Yet Truth-Seeking Advice \u2014 Neel Nanda",
    "url": "https://www.neelnanda.io/blog/51-socratic-persuasion",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "The full post is long, but you can 80/20 the value with the 700 word summary! Over half the post is eight optional case studies. Thanks to Jemima Jones, Claude 4 Opus and Gemini 2.5 Pro for help...",
    "full_snippet": "The full post is long, but you can 80/20 the value with the 700 word summary! Over half the post is eight optional case studies. Thanks to Jemima Jones, Claude 4 Opus and Gemini 2.5 Pro for help copy-editing and drafting TL;DR: I recommend giving advice\u00a0by asking questions\u00a0to walk someone through key steps in my argument\u00a0\u2014 often I\u2019m missing key info, which comes up quickly as an unexpected answer, while if I\u2019m right I\u2019m more persuasive. This error correction makes it safer to give opinionated advice, without overconfidence. This is useful in a wide range of settings, as a manager, managee, friend, and mentor, and is better for both parties, if you have the time and energy and are able to seriously engage with whether you\u00a0are wrong. Socratic Persuasion:\u00a0When trying to persuade\u00a0someone, especially if giving advice, I much prefer the Socratic method\u00a0over directly presenting my case.\u00a0I take my argument/thought process and break it down into 1-3 key step/cruxes, reframe each step into a que",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "What the humans like is responsiveness - by Sasha Chapin",
    "url": "https://sashachapin.substack.com/p/what-the-humans-like-is-responsiveness",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "What do the humans like? Apparently, they like this woman ordering food in a slightly flirtatious manner at a food truck. A total of 1.9 million souls have clicked \u201cheart\u201d on this brief clip. Okay,...",
    "full_snippet": "What do the humans like? Apparently, they like this woman ordering food in a slightly flirtatious manner at a food truck. A total of 1.9 million souls have clicked \u201cheart\u201d on this brief clip. Okay, sure\u2014on TikTok, that\u2019s not a completely unusual number. But it\u2019s a much more enthusiastic response than the other videos on this channel get, and the comments section reveals an unlikely fervor. \u201cI wish man,\u201d one guy comments, \u201cI can only wish that God would bless me with a girl like this. I\u2019d do anything for her bro.\u201d Another remark: \u201cI\u2019ve never been so in love in this app on my life.\u201d Is it that she\u2019s pretty? That can\u2019t be it entirely. I\u2019m aware of equally pretty women on the internet who try to get people to fall in love with them because it\u2019s their job\u2014influencers, porn stars, streamers, and so on. With all of their industriousness, charisma, and symmetricality, they do not get this kind of reception. And many of them are naked. What sets this person apart? It\u2019s simple: her insane respon",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Discovery fiction",
    "url": "https://michaelnotebook.com/df/index.html",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "To help me understand a scientific result, I often find it helpful to write what I call discovery fiction. By this I mean: a plausible story of how I could have discovered that result \u2013 an arc of...",
    "full_snippet": "To help me understand a scientific result, I often find it helpful to write what I call discovery fiction. By this I mean: a plausible story of how I could have discovered that result \u2013 an arc of small questions and ideas, false starts and backtracking, incremental steps eventually leading to the result. None of these things should come out of thin air: they should all be simple, almost-obvious steps. I've written a lot of discovery fiction. For instance, I have a Twitter thread on \"discovering\" the Hindu-Arabic numerals. And another Twitter thread on \"discovering\" quantum teleportation. I have discovery fiction essays about how to \"discover\" Bitcoin and how to \"discover\" the Bloom filter data structure. I've occasionally included discovery fiction inside longer books and essays. For example, section 6.2 of my first quantum computing book (jointly written with Ike Chuang) explains how the reader could plausibly have \"discovered\" the quantum search algorithm. Although it perhaps seems a",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "AI 2027",
    "url": "https://ai-2027.com",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "A research-backed AI scenario forecast.",
    "full_snippet": "A research-backed AI scenario forecast.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Intelligence Curse",
    "url": "https://intelligence-curse.ai",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "This series examines the incoming crisis of human irrelevance and provides a map towards a future where people remain the masters of their destiny.",
    "full_snippet": "This series examines the incoming crisis of human irrelevance and provides a map towards a future where people remain the masters of their destiny.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "DeepSeek's mHC: When Residual Connections Explode - Taylor Kolasinski",
    "url": "https://taylorkolasinski.com/notes/mhc-reproduction",
    "date": "January 13, 2026",
    "domain": "",
    "snippet": "Taylor Kolasinski - Engineering at FlowMode. ML systems & research, reinforcement learning, robotics. Based in Brooklyn, NY.",
    "full_snippet": "Taylor Kolasinski - Engineering at FlowMode. ML systems & research, reinforcement learning, robotics. Based in Brooklyn, NY.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "[2008.03936] Intelligent Matrix Exponentiation",
    "url": "https://arxiv.org/abs/2008.03936",
    "date": "January 12, 2026",
    "domain": "",
    "snippet": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and...",
    "full_snippet": "arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website. Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them. Have an idea for a project that will add value for arXiv's community? Learn more about arXivLabs. arXiv Operational Status \n",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "He Co-Invented the Transformer. Now: Continuous Thought Machines [Llion Jones / Luke Darlow] - YouTube",
    "url": "https://www.youtube.com/watch?v=DtePicx_kFY",
    "date": "January 12, 2026",
    "domain": "",
    "snippet": "A Transformer inventor shifts focus, exploring novel recurrent models. This Machine Learning Street Talk episode delves into the Continuous Thought Machine, a biologically-inspired architecture....",
    "full_snippet": "A Transformer inventor shifts focus, exploring novel recurrent models. This Machine Learning Street Talk episode delves into the Continuous Thought Machine, a biologically-inspired architecture. Learn about its unique neuron design and synchronization-based representation. Follow along using the transcript. A Transformer inventor shifts focus, exploring novel recurrent models. This Machine Learning Street Talk episode delves into the Continuous Thought Machine, a biologically-inspired architecture. Learn about its unique neuron design and synchronization-based representation. Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Understanding Image Gradients ",
    "url": "https://theailearner.com/2019/05/11/understanding-image-gradients",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "In the previous blogs, we discussed different smoothing filters. Before moving forward, let\u2019s first discuss Image Gradients which will be useful in edge detection, robust feature and texture...",
    "full_snippet": "In the previous blogs, we discussed different smoothing filters. Before moving forward, let\u2019s first discuss Image Gradients which will be useful in edge detection, robust feature and texture matching. So, let\u2019s first recall what a gradient is. In mathematics, the term gradient of a function means how a function is changing wrt. its arguments or independent variables. The gradient term is more frequently used for multi-variable functions. For a single variable function, we refer to this as the slope. The\u00a0gradient\u00a0of an N-variable function at each point is an N-D\u00a0vector\u00a0with the components given by the\u00a0derivatives\u00a0in the N-directions. e.g. for a 3-variable function (f(x,y,z)), the gradient, if it exists, is given by Thus, the gradient provides two pieces of information \u2013 magnitude and direction. The direction of the gradient tells us the direction of greatest increase while the magnitude represents the rate of increase in that direction. Because gradients are defined only for continuous ",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": " \"Autoregressive Transformers vs Text Diffusion Models\" / X",
    "url": "https://x.com/rasbt/status/2010376305720594810",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "To view keyboard shortcuts, press question mark\nView keyboard shortcuts\nAdded to your Bookmarks\nAdd to Folder\nHome\nExplore\nNotifications\nChat\nGrok\nBookmarks\nCreator...",
    "full_snippet": "To view keyboard shortcuts, press question mark\nView keyboard shortcuts\nAdded to your Bookmarks\nAdd to Folder\nHome\nExplore\nNotifications\nChat\nGrok\nBookmarks\nCreator Studio\nCommunities\nPremium\nProfile\nMore\nPost\nweeye\n@weeyev\nArticle\nReply\nSee new posts\nConversation\nSebastian Raschka\n@rasbt\nAutoregressive Transformers vs Text Diffusion Models\n3\n4\n31\n1.2K\nSomething I've been asked a lot in recent weeks is whether we see alternatives to the autoregressive transformer architecture (standard LLMs) in 2026. For now, I strongly believe that the transformer is here to stay for state-of-the-art modeling performance (for at least one to a few more years).\nBut yes, it will change a bit. Towards the end of the year, we saw a stronger focus towards hybrid architectures and making it more efficient. This is, of course, not a new idea, but recent releases from flagship labs show that there's now a stronger focus on it. (E.g., see Qwen3-Next, Kimi Linear, Nvidia Nemotron 3, DeepSeek V3.2 with sparse at",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Don't fall into the anti-AI hype - <antirez>",
    "url": "https://antirez.com/news/158",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "<antirez>\nDon't fall into the anti-AI hype\nantirez 5 hours ago. 36621 views.\nI love writing software, line by line. It could be said that my career was a continuous effort to create software well...",
    "full_snippet": "<antirez>\nDon't fall into the anti-AI hype\nantirez 5 hours ago. 36621 views.\nI love writing software, line by line. It could be said that my career was a continuous effort to create software well written, minimal, where the human touch was the fundamental feature. I also hope for a society where the last are not forgotten. Moreover, I don't want AI to economically succeed, I don't care if the current economic system is subverted (I could be very happy, honestly, if it goes in the direction of a massive redistribution of wealth). But, I would not respect myself and my intelligence if my idea of software and society would impair my vision: facts are facts, and AI is going to change programming forever.\n\nIn 2020 I left my job in order to write a novel about AI, universal basic income, a society that adapted to the automation of work facing many challenges. At the very end of 2024 I opened a YouTube channel focused on AI, its use in coding tasks, its potential social and economical effects",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "First-order Derivative kernels for Edge Detection | TheAILearner",
    "url": "https://theailearner.com/2019/05/24/first-order-derivative-kernels-for-edge-detection",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "Remember that derivatives only exists for continuous functions but the image is a discrete 2D light intensity function. Thus in the last blog, we approximated the image gradients using finite...",
    "full_snippet": "Remember that derivatives only exists for continuous functions but the image is a discrete 2D light intensity function. Thus in the last blog, we approximated the image gradients using finite approximation as For the edge detection case, we will prefer the central difference as shown above. Using this central difference, we can obtain the derivative filter in x and y directions as shown below Here, we have assumed that the\u00a0x-coordinate is increasing in the \u201cright\u201d-direction, and\u00a0y-coordinate in the \u201cdown\u201d-direction. By weighting these x and y derivatives, we can obtain different edge detection filters. Let\u2019s see how This is obtained by multiplying the x, and y-derivative filters obtained above with some smoothing filter(1D) in the other direction. For example, a 3\u00d73 Sobel-x and Sobel-y filter can be obtained as As we know that the Gaussian filter is used for blurring thus, the Sobel operator computes the gradient with smoothing. Thus this is less sensitive to noise. Because of separabi",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Using AI, Mathematicians Find Hidden Glitches in Fluid Equations | Quanta Magazine",
    "url": "https://www.quantamagazine.org/using-ai-mathematicians-find-hidden-glitches-in-fluid-equations-20260109",
    "date": "January 11, 2026",
    "domain": "",
    "snippet": "Nearly 200 years ago, the physicists Claude-Louis Navier and George Gabriel Stokes put the finishing touches on a set of equations that describe how fluids swirl. And for nearly 200 years, the...",
    "full_snippet": "Nearly 200 years ago, the physicists Claude-Louis Navier and George Gabriel Stokes put the finishing touches on a set of equations that describe how fluids swirl. And for nearly 200 years, the Navier-Stokes equations have served as an unimpeachable theory of how fluids in the real world behave \u2014 from ocean currents threading their way between the continents to air wrapping around an aircraft\u2019s wings. Nevertheless, many mathematicians suspect that glitches hide deep within the equations. They have a hunch that in certain situations, the theory fails. In these cases, the equations will predict a fluid moving in some unphysical, incomprehensible way \u2014 spinning into an impossibly fast vortex, for instance, or instantly reversing its flow. Some quantity in the equations will grow infinitely large, or \u201cblow up,\u201d as mathematicians put it. Despite immense effort, no one has been able to come up with a situation where the Navier-Stokes equations falter. Doing so \u2014 or, alternatively, proving tha",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Genie: Generative Interactive Environments",
    "url": "https://arxiv.org/pdf/2402.15391",
    "date": "January 8, 2026",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Chandrasekhar\u2019s Voyage into Black Holes",
    "url": "https://mathsciencehistory.com/flashcards-friday-chandrasekhars-voyage-into-black-holes",
    "date": "January 8, 2026",
    "domain": "",
    "snippet": "Wel\u00adcome to Flash\u00adcards Fri\u00adday here at Math! Sci\u00adence! His\u00adto\u00adry! where every Fri\u00adday, we take a lit\u00adtle idea and make a big dis\u00adcov\u00adery out of\u00a0it. I\u2019m your host, Gabrielle Bir\u00adchak, and today\u2019s...",
    "full_snippet": "Wel\u00adcome to Flash\u00adcards Fri\u00adday here at Math! Sci\u00adence! His\u00adto\u00adry! where every Fri\u00adday, we take a lit\u00adtle idea and make a big dis\u00adcov\u00adery out of\u00a0it. I\u2019m your host, Gabrielle Bir\u00adchak, and today\u2019s sto\u00adry is about a young sci\u00aden\u00adtist, a long, relax\u00ading, boat ride, and a rev\u00ade\u00adla\u00adtion that changed the way we under\u00adstand the death of stars, and the birth of black\u00a0holes. The year was 1930. A young man named Sub\u00adrah\u00admanyan Chan\u00addrasekhar, just 19 years old, was board\u00ading a ship called the S.S. Pil\u00adsna in Bom\u00adbay, India, head\u00aded for England. The voy\u00adage would take about two weeks, a slow cross\u00ading of the Indi\u00adan Ocean, through the Suez Canal, across the Mediter\u00adranean, and final\u00adly toward the Eng\u00adlish Chan\u00adnel.\nNo phones. No inter\u00adnet. Just the ocean, his books, a note\u00adbook, and a whole lot of\u00a0time. Chan\u00addrasekhar wasn\u2019t plan\u00adning on rev\u00ado\u00adlu\u00adtion\u00adiz\u00ading astro\u00adphysics on this trip.\nHe sim\u00adply want\u00aded to get to Cam\u00adbridge Uni\u00adver\u00adsi\u00adty, where he\u2019d begin his grad\u00adu\u00adate studies. But some\u00adwhere b",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Thinking About a Bias for Better Semantic Coherence and Reuse - jchencxh",
    "url": "https://jchencxh.github.io/blog/better-coherence",
    "date": "January 8, 2026",
    "domain": "",
    "snippet": "For a general intelligence, good generalisation requires good reuse of semantic components. Good reuse of semantics requires that there exists a consistent internal handle for the same semantic...",
    "full_snippet": "For a general intelligence, good generalisation requires good reuse of semantic components. Good reuse of semantics requires that there exists a consistent internal handle for the same semantic purpose across contexts (e.g., a stable readout for classification from a linear layer or stable causal role in the computation) so that downstream computations can depend on \u201cthe same thing\u201d reliably even if the internal basis is distributed or not uniquely identifiable. This typically requires some notion of (conditional) invariance or equivariance, which could be that the concept\u2019s identity should be stable under semantics-preserving transformations, and when a transformation should matter (e.g., location), the representation should change in a predictable, structured way. Thus good reuse of semantic components requires that the components themselves have a notion of semantic invariance and are likely to have relationships that reflect the underlying reality it observes. I\u2019m going to refer to",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How not to do research - Rajan Agarwal",
    "url": "https://www.rajan.sh/multiplayer",
    "date": "January 8, 2026",
    "domain": "",
    "snippet": "I don't usually share when things go wrong. Like most, my public work tends to be the stuff that worked, but I learned a lot from this project, and I want to share what I learned about the problem,...",
    "full_snippet": "I don't usually share when things go wrong. Like most, my public work tends to be the stuff that worked, but I learned a lot from this project, and I want to share what I learned about the problem, and how not to do research. I tried to train a Genie-style world model that could learn to segment two players and their independent action spaces from Pong video alone, without labels or hardcoded structure. The model kept collapsing to degenerate solutions. It would always track the ball as \"player 1\" or even treat the score as an agent. I eventually realized I was trying to force behavior, ignoring the core problem that unsupervised multi-actor discovery requires data diversity and scale. With only one game type and a single camera angle, there's no modelling pressure to learn the \"right\" segmentation. Any learned decomposition that enables good reconstruction is equally valid. But I should have known this before I wrote any training code. I spent weeks adding designing custom loss functi",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Braitenberg vehicle - Wikipedia",
    "url": "https://en.wikipedia.org/wiki/Braitenberg_vehicle",
    "date": "January 7, 2026",
    "domain": "",
    "snippet": "A Braitenberg vehicle is a concept presented as a thought experiment by the Italian cyberneticist Valentino Braitenberg in his book Vehicles: Experiments in Synthetic Psychology. The book models the...",
    "full_snippet": "A Braitenberg vehicle is a concept presented as a thought experiment by the Italian cyberneticist Valentino Braitenberg in his book Vehicles: Experiments in Synthetic Psychology. The book models the animal world in a minimalistic and constructive way, from simple reactive behaviours (like phototaxis) through the simplest vehicles, to the formation of concepts, spatial behaviour, and generation of ideas. For the simplest vehicles, the motion of the vehicle is directly controlled by some sensors (for example photo cells). Yet the resulting behaviour may appear complex or even intelligent. A Braitenberg vehicle is an agent that can autonomously move around based on its sensor inputs. It has primitive sensors that measure some stimulus at a point, and wheels (each driven by its own motor) that function as actuators or effectors. In the simplest configuration, a sensor is directly connected to an effector, so that a sensed signal immediately produces a movement of the wheel. Depending on ho",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Radial Basis Function ",
    "url": "https://www.appliedaicourse.com/blog/radial-basis-function-in-machine-learning",
    "date": "January 7, 2026",
    "domain": "",
    "snippet": "Radial Basis Functions (RBF) play an essential role in Machine Learning, particularly in addressing non-linear problems. They are used to approximate complex functions, classify data, and solve...",
    "full_snippet": "Radial Basis Functions (RBF) play an essential role in Machine Learning, particularly in addressing non-linear problems. They are used to approximate complex functions, classify data, and solve regression tasks efficiently. RBFs became popular in the late 1980s when Broomhead and Lowe introduced RBF Neural Networks, offering a new way to handle non-linear relationships in data. Since then, RBF networks have been widely applied in areas like pattern recognition, time-series prediction, and control systems. In this article, we will break down the concept of Radial Basis Functions, their architecture, training process, and applications in a simple, beginner-friendly way. A Radial Basis Function (RBF) is a type of mathematical function whose value depends only on the distance from a central point. In Machine Learning, RBFs are commonly used to model non-linear relationships, making them effective for tasks like function approximation, classification, and regression. The Radial Basis Functi",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "REAP- model pruning ",
    "url": "https://www.cerebras.ai/blog/reap",
    "date": "January 5, 2026",
    "domain": "",
    "snippet": "Cerebras Systems and U.S. Department of Energy Sign MOU to Accelerate the Genesis Mission and U.S. National AI Initiative. Read more >> OCT 16 2025 TL;DR: We introduce REAP (Router-weighted Expert...",
    "full_snippet": "Cerebras Systems and U.S. Department of Energy Sign MOU to Accelerate the Genesis Mission and U.S. National AI Initiative. Read more >> OCT 16 2025 TL;DR: We introduce REAP (Router-weighted Expert Activation Pruning), a new one-shot method for compressing Mixture-of-Experts (MoE) language models. Our key finding is that for generative tasks like code generation pruning low-impact experts is fundamentally better than merging them. REAP removes up to 50% of experts from models as large as 1 trillion parameters while largely maintaining baseline model quality. For instance, with the\u00a0Qwen3-480B-Coder-FP8\u00a0model, REAP at 50% pruning retains 97.6% of its baseline non-agentic coding ability and 96.7% on the agentic SWE-Bench benchmark. We are open-sourcing the complete codebase and pruned model checkpoints on HuggingFace to encourage further research. Sparsely-activated Mixture-of-Experts (SMoE) models achieve their high quality by\u00a0decoupling their total parameter count from their computationa",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Hidden Mathematics in Stranger Things - YouTube",
    "url": "https://www.youtube.com/watch?v=_Y0smObqqhw",
    "date": "January 4, 2026",
    "domain": "",
    "snippet": "Ellie Sleightholm explores the mathematical concepts in Stranger Things 5. The video analyzes Dustin's circle calculations and delves into complex equations from general relativity. Learn about the...",
    "full_snippet": "Ellie Sleightholm explores the mathematical concepts in Stranger Things 5. The video analyzes Dustin's circle calculations and delves into complex equations from general relativity. Learn about the ADM split and Einstein's field equations, as they relate to wormholes. Follow along using the transcript. Ellie Sleightholm explores the mathematical concepts in Stranger Things 5. The video analyzes Dustin's circle calculations and delves into complex equations from general relativity. Learn about the ADM split and Einstein's field equations, as they relate to wormholes. Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Visualizing transformers and attention | Talk for TNG Big Tech Day '24 - YouTube",
    "url": "https://www.youtube.com/watch?v=KJtZARuO3JY",
    "date": "January 4, 2026",
    "domain": "",
    "snippet": "Follow along using the transcript. Follow along using the transcript. New New New New ",
    "full_snippet": "Follow along using the transcript. Follow along using the transcript. New New New New ",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "\u2018grokking (NN)\u2019 directory \u00b7 Gwern.net",
    "url": "https://gwern.net/doc/ai/scaling/emergence/grokking/index",
    "date": "January 3, 2026",
    "domain": "",
    "snippet": "Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond The slingshot helps with learning Emergent properties with repeated...",
    "full_snippet": "Deep Learning Through A Telescoping Lens: A Simple Model Provides Empirical Insights On Grokking, Gradient Boosting & Beyond The slingshot helps with learning Emergent properties with repeated examples Grokking Modular Polynomials Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks Grokfast: Accelerated Grokking by Amplifying Slow Gradients Deep Grokking: Would Deep Neural Networks Generalize Better? Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization Unified View of Grokking, Double Descent and Emergent Abilities: A Perspective from Circuits Competition A Tale of Tails: Model Collapse as a Change of Scaling Laws Critical Data Size of Language Models from a Grokking Perspective Grokking Group Multiplication with Cosets Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce Grokking Outliers with Opposing Signals Have an Outsized Effect on Neural Network Optimization Grokking Bey",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Claude Bliss Attractor - by Scott Alexander",
    "url": "https://www.astralcodexten.com/p/the-claude-bliss-attractor",
    "date": "January 2, 2026",
    "domain": "",
    "snippet": "...",
    "full_snippet": "...",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Position: Categorical Deep Learning is an Algebraic Theory of All Architectures",
    "url": "https://arxiv.org/pdf/2402.15332",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "John Carmack on Idea Generation",
    "url": "https://amasad.me/carmack",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "Last year at an internal talk at Facebook I was fortunate to see [John Carmack](https://en.wikipedia.org/wiki/John_Carmack) speak about his idea generation system. At first I was disappointed...",
    "full_snippet": "Last year at an internal talk at Facebook I was fortunate to see [John Carmack](https://en.wikipedia.org/wiki/John_Carmack) speak about his idea generation system. At first I was disappointed because...",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Category: The Essence of Composition | \u00a0\u00a0Bartosz Milewski's Programming Cafe",
    "url": "https://bartoszmilewski.com/2014/11/04/category-the-essence-of-composition",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "I was overwhelmed by the positive response to my previous post, the Preface to Category Theory for Programmers. At the same time, it scared the heck out of me because I realized what high...",
    "full_snippet": "I was overwhelmed by the positive response to my previous post, the Preface to Category Theory for Programmers. At the same time, it scared the heck out of me because I realized what high expectations people were placing in me. I\u2019m afraid that no matter what I\u2019ll write, a lot of readers will be disappointed. Some readers would like the book to be more practical, others more abstract. Some hate C++ and would like all examples in Haskell, others hate Haskell and demand examples in Java. And I know that the pace of exposition will be too slow for some and too fast for others. This will not be the perfect book. It will be a compromise. All I can hope is that I\u2019ll be able to share some of my aha! moments with my readers. Let\u2019s start with the basics. A category is an embarrassingly simple concept. A category consists of objects and arrows that go between them. That\u2019s why categories are so easy to represent pictorially. An object can be drawn as a circle or a point, and an arrow\u2026 is an arrow.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Mixture of Experts Explained",
    "url": "https://huggingface.co/blog/moe",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "With the release of Mixtral 8x7B (announcement, model card), a class of transformer has become the hottest topic in the open AI community: Mixture of Experts, or MoEs for short. In this blog post, we...",
    "full_snippet": "With the release of Mixtral 8x7B (announcement, model card), a class of transformer has become the hottest topic in the open AI community: Mixture of Experts, or MoEs for short. In this blog post, we take a look at the building blocks of MoEs, how they\u2019re trained, and the tradeoffs to consider when serving them for inference. Let\u2019s dive in! MoEs: Let\u2019s dive in! The scale of a model is one of the most important axes for better model quality. Given a fixed computing budget, training a larger model for fewer steps is better than training a smaller model for more steps. Mixture of Experts enable models to be pretrained with far less compute, which means you can dramatically scale up the model or dataset size with the same compute budget as a dense model. In particular, a MoE model should achieve the same quality as its dense counterpart much faster during pretraining. So, what exactly is a MoE? In the context of transformer models, a MoE consists of two main elements: So, to recap, in MoEs",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Futurist Manifesto, by Filippo Tommaso Marinetti",
    "url": "https://www.arthistoryproject.com/artists/filippo-tommaso-marinetti/the-futurist-manifesto",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "We had stayed up all night, my friends and I, under hanging mosque lamps with domes of filigreed brass, domes starred like our spirits, shining like them with the prisoned radiance of electric...",
    "full_snippet": "We had stayed up all night, my friends and I, under hanging mosque lamps with domes of filigreed brass, domes starred like our spirits, shining like them with the prisoned radiance of electric hearts. For hours we had trampled our atavistic ennui into rich oriental rugs, arguing up to the last confines of logic and blackening many reams of paper with our frenzied scribbling. An immense pride was buoying us up, because we felt ourselves alone at that hour, alone, awake, and on our feet, like proud beacons or forward sentries against an army of hostile stars glaring down at us from their celestial encampments. Alone with stokers feeding the hellish fires of great ships, alone with the black spectres who grope in the red-hot bellies of locomotives launched on their crazy courses, alone with drunkards reeling like wounded birds along the city walls. Suddenly we jumped, hearing the mighty noise of the huge double-decker trams that rumbled by outside, ablaze with colored lights, like village",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "returning to stanford | writing",
    "url": "https://masonjwang.com/writing/returning-to-stanford",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "Every reinvention story is a confession, and mine is this: before I started at Stanford this September, when I had often thought I would never return, I was very dismissive of what college had to...",
    "full_snippet": "Every reinvention story is a confession, and mine is this: before I started at Stanford this September, when I had often thought I would never return, I was very dismissive of what college had to offer. I had two gap years in my pockets: living in Rome with other founders; crying after firing someone for the first time; traveling the world, hiking in Uzbekistan, meeting my cofounder's family in Abu Dhabi; sneaking into Stanford dining halls. While I never admitted it, my mindset reeked of how proudly I had weaponized myself. My plan was to do one quarter at Stanford, learn the math that was bottlenecking me, methodically explore, then find a \u201crocketship\u201d to leave for. I finished my first quarter last week. It rushes past the same way: biking through campus feeling like a kid, laughing with new friends harder than I have in my life. I have found a magical place that makes it foolish to have ever considered not coming, and magical people here who make the thought of leaving even more foo",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Ithaka | The Poetry Foundation",
    "url": "https://www.poetryfoundation.org/poems/51296/ithaka-56d22eef917ec",
    "date": "January 1, 2026",
    "domain": "",
    "snippet": "At a Slight Angle to the Universe Advertise with Poetry Poetry Magazine Poetry Magazine Archive Submit to Poetry",
    "full_snippet": "At a Slight Angle to the Universe Advertise with Poetry Poetry Magazine Poetry Magazine Archive Submit to Poetry",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Project Gutenberg eBook #5740: Tractatus Logico-Philosophicus",
    "url": "https://www.gutenberg.org/files/5740/5740-pdf.pdf",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "N/A",
    "full_snippet": "N/A",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Shared State of Mathematics",
    "url": "https://vaibhawvipul.github.io/2025/12/30/FLT-helped-me-find-upper-planes.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Last month I wrote about failed attempt to understand Fermat\u2019s Last Theorem. I confessed failure \u2014 or at least, incomplete success. I understood the skeleton of Wiles\u2019 proof, but not its flesh. But...",
    "full_snippet": "Last month I wrote about failed attempt to understand Fermat\u2019s Last Theorem. I confessed failure \u2014 or at least, incomplete success. I understood the skeleton of Wiles\u2019 proof, but not its flesh. But something unplanned happened, I stumbled into something I wasn\u2019t looking for: a hidden architecture beneath mathematics itself. Modular Forms, Elliptic Curves, and Hyperbolic Geometry previously seemed like something I might never understand them in this lifetime. And at the center of it all, I kept finding the same object. Again and again. \u210d, the upper half-plane: where . Positive imaginary part. I started with Silverman\u2019s Rational Points on Elliptic Curves, because that\u2019s where FLT proof by Wiles had pointed initially. Elliptic curves over \u2102 are tori - quotients of the complex plane by lattices. At first this felt like a technical detail. Then I realized that every lattice is encoded by a single complex number with positive imaginary part. In other words, by a point in the upper half-plane",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "What's Our Problem? \u2014 Wait But Why",
    "url": "https://waitbutwhy.com",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "A popular long-form, stick-figure-illustrated blog about almost everything.",
    "full_snippet": "A popular long-form, stick-figure-illustrated blog about almost everything.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Petri Dish Neural Cellular Automata",
    "url": "https://pub.sakana.ai/pdnca",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "While neural cellular automata (NCA) have proven effective for modeling morphogenesis and self-organizing processes, they are typically governed by a fixed, non-adaptive update rule shared across all...",
    "full_snippet": "While neural cellular automata (NCA) have proven effective for modeling morphogenesis and self-organizing processes, they are typically governed by a fixed, non-adaptive update rule shared across all cells. Each cell applies the same learned local transition function throughout its lifetime, resulting in static developmental dynamics once training is complete. We introduce Petri Dish Neural Cellular Automata (PD-NCA), a differentiable Artificial Life substrate that removes this constraint by allowing multiple, independent NCA agents to coexist, compete, and adapt within a shared environment. Unlike conventional NCA, each agent in PD-NCA continually updates its parameters via gradient descent during the simulation itself, enabling within-lifetime learning and open-ended behavioral change. This continual, multi-agent learning process transforms morphogenesis from a fixed developmental program into a dynamic ecosystem of interacting, adaptive entities. Through these interactions, PD-NCA e",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "a journey through the american west - by Jasmine Li",
    "url": "https://jasminexli.substack.com/p/a-journey-through-the-american-west",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Taking a long journey on low-speed rail has for a while been on my bucket list.1 So, I organized a trip with friends this Thanksgiving break on the California Zephyr, which runs from Chicago to San...",
    "full_snippet": "Taking a long journey on low-speed rail has for a while been on my bucket list.1 So, I organized a trip with friends this Thanksgiving break on the California Zephyr, which runs from Chicago to San Francisco over 50+ hours. Enjoy! On Thursday, I take the bus from Ithaca to NYC, then an evening flight to Chicago, all to just reach the starting location of the Zephyr. This might be my first experience traveling exclusively to take another mode of transport. We crash at Jo\u2019s place that night, though I end up getting zero sleep \u2014 catching up, learning to juggle and do jiu-jitsu, and talking about research delightfully gets in the way! But fighting delirium from the all-nighter, we make it to Union Station and board the 2PM Zephyr on Friday with ~10 minutes to spare. The four of us taking the train together are seated in the same row on the final upper-level car. Almost immediately we are introduced on the announcement speaker to Ms. Yolanda, manager of the on-train cafe, who describes the ",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Deep-learning model predicts how fruit flies form, cell by cell | MIT News | Massachusetts Institute of Technology",
    "url": "https://news.mit.edu/2025/deep-learning-model-predicts-how-fruit-flies-form-1215",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n    Creative Commons Attribution Non-Commercial No Derivatives...",
    "full_snippet": "Images for download on the MIT News office website are made available to non-commercial entities, press and the general public under a \n    Creative Commons Attribution Non-Commercial No Derivatives license.\n    You may not alter the images provided, other than to crop them to size. A credit line must be used when reproducing images; if one is not provided \n    below, credit the images to \"MIT.\" During early development, tissues and organs begin to bloom through the shifting, splitting, and growing of many thousands of cells. A team of MIT engineers has now developed a way to predict, minute by minute, how individual cells will fold, divide, and rearrange during a fruit fly\u2019s earliest stage of growth. The new method may one day be applied to predict the development of more complex tissues, organs, and organisms. It could also help scientists identify cell patterns that correspond to early-onset diseases, such as asthma and cancer. In a study appearing today in the journal Nature Method",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Transformers and Self-Attention (DL 19) - YouTube",
    "url": "https://www.youtube.com/watch?v=e9-0BxyKG10",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "This Davidson College lecture explores Transformer neural networks, explaining their architecture and self-attention mechanisms. Professor Bryce details how these networks process word embeddings and...",
    "full_snippet": "This Davidson College lecture explores Transformer neural networks, explaining their architecture and self-attention mechanisms. Professor Bryce details how these networks process word embeddings and leverage residual connections. The video uses diagrams to illustrate the flow of data and the role of \"query,\" \"key,\" and \"value\" layers. 0 of 26 lessons complete Follow along using the transcript. This Davidson College lecture explores Transformer neural networks, explaining their architecture and self-attention mechanisms. Professor Bryce details how these networks process word embeddings and leverage residual connections. The video uses diagrams to illustrate the flow of data and the role of \"query,\" \"key,\" and \"value\" layers. 0 of 26 lessons complete Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Towards Geometric Deep Learning",
    "url": "https://thegradient.pub/towards-geometric-deep-learning",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Geometric Deep Learning is an umbrella term for approaches considering a broad class of ML problems from the perspectives of symmetry and invariance. It provides a common blueprint allowing to derive...",
    "full_snippet": "Geometric Deep Learning is an umbrella term for approaches considering a broad class of ML problems from the perspectives of symmetry and invariance. It provides a common blueprint allowing to derive from first principles neural network architectures as diverse as CNNs, GNNs, and Transformers. Here, we study how these ideas have emerged through history from ancient Greek geometry to Graph Neural Networks. The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach \u2014 computer vision, playing Go, or protein folding \u2014 are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, ",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Consciousness and Philosophy of Mind",
    "url": "https://www.doc.ic.ac.uk/~mpsha/consciousness_and_philosophy.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Understanding consciousness is one of the most profound intellectual challenges we face, and with the advent of increasingly compelling AI, it has assumed practical signnificance. I have been working...",
    "full_snippet": "Understanding consciousness is one of the most profound intellectual challenges we face, and with the advent of increasingly compelling AI, it has assumed practical signnificance. I have been working on the topic since the mid-2000s. My conviction is that to arrive at a proper understanding of consciousness we need to engage with philosophy as well as carrying out empirical work in psychology and neuroscience. On the philosophical front, my work has been much influenced by the later writings of Wittgenstein, which I see as an antidote to our natural dualistic tendencies. The upshot is that, rather than asking what consciousness *is* (and placing a heavy metaphysical burden on that word), we should ask how words like \"consciousness\" are used. The most detailed exposition of this idea, which excavates the very roots of philosophy, is in Chapter 1 of my 2010 book \"Embodiment and the Inner Life\". But the first paper I published on the theme was \"Global Access, Embodiment, and the Conscious",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Story of Heads",
    "url": "https://lena-voita.github.io/posts/acl19_heads.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on OpenSubtitles EN-RU...",
    "full_snippet": "Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on OpenSubtitles EN-RU Model trained on OpenSubtitles EN-RU subject-> verb verb -> subject subject-> verb verb -> subject verb -> subject object -> verb verb -> object object -> verb Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-DE Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-FR Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on WMT EN-RU Model trained on OpenSubtitles EN-RU Model trained on OpenSubtitles EN-RU Model trained on OpenSubtitles EN-RU Last updated June 12, 2025.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "World Models",
    "url": "https://worldmodels.github.io",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Can agents learn inside of their own dreams?",
    "full_snippet": "Can agents learn inside of their own dreams?",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Neural Networks, Manifolds, and Topology -- colah's blog",
    "url": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Recently, there\u2019s been a great deal of excitement and interest in deep neural networks because they\u2019ve achieved breakthrough results in areas such as computer vision.1",
    "full_snippet": "Recently, there\u2019s been a great deal of excitement and interest in deep neural networks because they\u2019ve achieved breakthrough results in areas such as computer vision.1",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How To Win",
    "url": "https://planetbanatt.net/articles/gametypes.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "I really like games. I think that they provide a nice controlled environment for distilling what it means to become good at something, and as a result I think that everybody should play them. Most...",
    "full_snippet": "I really like games. I think that they provide a nice controlled environment for distilling what it means to become good at something, and as a result I think that everybody should play them. Most games will generally have a common thread connecting them, and learning to get good at one will usually provide you with a framework that should enable you to get good at any of them. I wanted to talk about some concepts that are important for reaching a moderately high level at games in general. Specifically, I want to pull in various advice on learning games from all sorts of genres in order to illustrate the common thread that connects all of it together, even if on the surface everything seems really different. The number one reason people do not improve at games is because of self-imposed restrictions they place upon themselves. In general, it's pretty easy to figure out when things are strong very soon after playing: you will be defeated by them many times, and they will begin to reveal",
    "to_read": false,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How the NanoGPT Speedrun WR dropped by 20% in 3 months \u2014 LessWrong",
    "url": "https://www.lesswrong.com/posts/j3gp8tebQiFJqzBgg/how-the-nanogpt-speedrun-wr-dropped-by-20-in-3-months",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "In early 2024 Andrej Karpathy stood up an llm.c repo to train GPT-2 (124M), which took an equivalent of 45 minutes on 8xH100 GPUs to reach 3.28 cross entropy loss. By Jan 2025, collaborators of...",
    "full_snippet": "In early 2024 Andrej Karpathy stood up an llm.c repo to train GPT-2 (124M), which took an equivalent of 45 minutes on 8xH100 GPUs to reach 3.28 cross entropy loss. By Jan 2025, collaborators of modded-nanogpt brought that time down to 3 minutes. It sat near 3 minutes until July 2025, having a large swath of optimization already applied: RoPE, value embeddings, reduce scatter grad updates, Muon, QK Norm, Relu^2, a custom FP8 head, skip connections, flex attention, short-long windows, attention window warmup, linear lr cooldown, and more. Yet, in the last 3 months the record has fallen by another \u00a020% to 2 minutes and 20 seconds. Many of the improvements in the last 20% have not yet been published outside of the modded-nanogpt repo. This post summarizes those improvements. Not everything will generalize to larger scales, but there are some core concepts that I believe are promising. Improvements are sorted into ML and Engineering, grouped by concept, and subjectively ranked by their gene",
    "to_read": false,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "How to Think About GPUs | How To Scale Your Model",
    "url": "https://jax-ml.github.io/scaling-book/gpus",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We love TPUs at Google, but GPUs are great too. This chapter takes a deep dive into the world of NVIDIA GPUs \u2013 how each chip works, how they\u2019re networked together, and what that means for LLMs,...",
    "full_snippet": "We love TPUs at Google, but GPUs are great too. This chapter takes a deep dive into the world of NVIDIA GPUs \u2013 how each chip works, how they\u2019re networked together, and what that means for LLMs, especially compared to TPUs. This section builds on Chapter 2 and Chapter 5, so you are encouraged to read them first. Jacob Austin\u2020 \u2020Google DeepMind Swapnil Patil\u2020 Adam Paszke\u2020 Reiner Pope* *MatX Aug. 18, 2025 A modern ML GPU (e.g. H100, B200) is basically a bunch of compute cores that specialize in matrix multiplication (called Streaming Multiprocessors or SMs) connected to a stick of fast memory (called HBM). Here\u2019s a diagram: Each SM, like a TPU\u2019s Tensor Core, has a dedicated matrix multiplication core (unfortunately also called a Tensor Core), a vector arithmetic unit (called a Warp Scheduler), and a fast on-chip cache (called SMEM). Unlike a TPU, which has at most 2 independent \u201cTensor Cores\u201d, a modern GPU has more than 100 SMs (132 on an H100). Each of these SMs is much less powerful than",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Bahdanau Attention Mechanism - MachineLearningMastery.com",
    "url": "https://machinelearningmastery.com/the-bahdanau-attention-mechanism",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Conventional encoder-decoder architectures for machine translation encoded every source sentence into a fixed-length vector, regardless of its length, from which the decoder would then generate a...",
    "full_snippet": "Conventional encoder-decoder architectures for machine translation encoded every source sentence into a fixed-length vector, regardless of its length, from which the decoder would then generate a translation. This made it difficult for the neural network to cope with long sentences, essentially resulting in a performance bottleneck. The Bahdanau attention was proposed to address the performance bottleneck of conventional encoder-decoder architectures, achieving significant improvements over the conventional approach. In this tutorial, you will discover the Bahdanau attention mechanism for neural machine translation. After completing this tutorial, you will know: Kick-start your project with my book Building Transformer Models with Attention. It provides self-study tutorials with working code to guide you into building a fully-working transformer model that can\ntranslate sentences from one language to another... Let\u2019s get started. The Bahdanau attention mechanism\nPhoto by Sean Oulashin,",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "KV Caching & Attention Optimization: From O(n\u00b2) to O(n) | by pdawg | Medium",
    "url": "https://medium.com/@prathamgrover777/kv-caching-attention-optimization-from-o-n%C2%B2-to-o-n-8b605f0d4072",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We\u2019ve seen how an LLM types out a thousand-word answer, word by word, as if it\u2019s \u201cthinking out loud\u201d. It feels smooth, but behind the scenes, the process is painfully inefficient. At generation step...",
    "full_snippet": "We\u2019ve seen how an LLM types out a thousand-word answer, word by word, as if it\u2019s \u201cthinking out loud\u201d. It feels smooth, but behind the scenes, the process is painfully inefficient. At generation step t, the model must ensure the next word is consistent with everything that came before: your prompt, the partial output so far, system instructions, or any hidden context. In practical terms, the model rebuilds all previous hidden states through all its transformer layers and computes Queries, Keys, and Values again, even though the previous tokens haven\u2019t changed at all. This re-computation happens layer by layer and head by head. Nothing is reused. As shown in Fig. 1, the computation per token keeps increasing with sequence length because nothing from previous steps is reused. Why This Scales So Badly? Imagine writing a paragraph, and before you add each new sentence, you re-read the entire document from the beginning. Then you do it again for the next sentence. and again. and again. that\u2019",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Transformers are Graph Neural Networks",
    "url": "https://thegradient.pub/transformers-are-graph-neural-networks",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "My engineering friends often ask me: deep learning on graphs sounds great, but are there any real applications? While Graph Neural Networks are used in recommendation systems at Pinterest, Alibaba...",
    "full_snippet": "My engineering friends often ask me: deep learning on graphs sounds great, but are there any real applications? While Graph Neural Networks are used in recommendation systems at Pinterest, Alibaba and Twitter, a more subtle success story is the Transformer architecture, which has taken the NLP world by storm. Through this post, I want to establish a link between Graph Neural Networks (GNNs) and Transformers. I'll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we can work together to drive future progress. Let's start by talking about the purpose of model architectures\u2014representation learning. At a high level, all neural network architectures build representations of input data as vectors/embeddings, which encode useful statistical and semantic information about the data. These latent or hidden representations can then be used for performing something useful, such as classifying an image ",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Using topology for discrete problems | The Borsuk-Ulam theorem and stolen necklaces - YouTube",
    "url": "https://www.youtube.com/watch?v=yuVqxCSsE7c",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Follow along using the transcript. Follow along using the transcript.",
    "full_snippet": "Follow along using the transcript. Follow along using the transcript.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Energy-Based Models \u00b7 Deep Learning",
    "url": "https://atcold.github.io/NYU-DLSP20/en/week07/07-1",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We will introduce a new framework for defining models. It provides a unifying umbrella that helps define supervised, unsupervised and self-supervised models. Energy-based models observe a set of...",
    "full_snippet": "We will introduce a new framework for defining models. It provides a unifying umbrella that helps define supervised, unsupervised and self-supervised models. Energy-based models observe a set of variables \n\ud835\udc65\nx and output a set of variables \n\ud835\udc66\ny. There are 2 major problems with feed-forward nets: Instead of trying to classify \n\ud835\udc65\nx\u2019s to \n\ud835\udc66\ny\u2019s, we would like to predict if a certain pair of (\n\ud835\udc65\nx, \n\ud835\udc66\ny) fit together or not. Or in other words, find a \n\ud835\udc66\ny compatible with \n\ud835\udc65\nx. We can also pose the problem as finding a \n\ud835\udc66\ny for which some \n\ud835\udc39\n(\n\ud835\udc65\n,\n\ud835\udc66\n)\nF(x,y) is low. For example: We define an energy function \n\ud835\udc39\n:\n\ud835\udc4b\n\u00d7\n\ud835\udc4c\n\u2192\n\ud835\udc45\nF:X\u00d7Y\u2192R where \n\ud835\udc39\n(\n\ud835\udc65\n,\n\ud835\udc66\n)\nF(x,y) describes the level of dependency between \n(\n\ud835\udc65\n,\n\ud835\udc66\n)\n(x,y) pairs. (Note that this energy is used in inference, not in learning.) The inference is given by the following equation: We would like the energy function to be smooth and differentiable so that we can use it to perform the gradient-based method for inference. I",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "On the Biology of a Large Language Model",
    "url": "https://transformer-circuits.pub/2025/attribution-graphs/biology.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We investigate the internal mechanisms used by Claude 3.5 Haiku \u2014 Anthropic's lightweight production model \u2014 in a variety of contexts, using our circuit tracing methodology.",
    "full_snippet": "We investigate the internal mechanisms used by Claude 3.5 Haiku \u2014 Anthropic's lightweight production model \u2014 in a variety of contexts, using our circuit tracing methodology.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Galaxy brain resistance",
    "url": "https://vitalik.eth.limo/general/2025/11/07/galaxybrain.html",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "One important property for a style of thinking and argumentation to have is what I call galaxy brain resistance: how difficult is it to abuse that style of thinking to argue for pretty much whatever...",
    "full_snippet": "One important property for a style of thinking and argumentation to have is what I call galaxy brain resistance: how difficult is it to abuse that style of thinking to argue for pretty much whatever you want - something that you already decided elsewhere for other reasons? The spirit here is similar to falsifiability in science: if your arguments can justify anything, then your arguments imply nothing. You want to get to step 2 and then stop. It's easiest to motivate the need to think about galaxy brain resistance by looking at what happens in its absence. You've probably heard many cases of people saying things like this: We are building a new decentralized ____ marketplace that will revolutionize how ____ customers interact with their providers, and will allow creators to turn their audiences into digital nation states. $____ is a governance token that lets you play a direct role in this rapidly growing market. If we capture even 1% of the ___ market share, this will be a $___ billio",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "On Seeing Through and Unseeing: The Hacker Mindset \u00b7 Gwern.net",
    "url": "https://gwern.net/unseeing",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Defining the security/hacker mindset as extreme reductionism: ignoring the surface abstractions and limitations to treat a system as a source of parts to manipulate into a different system, with...",
    "full_snippet": "Defining the security/hacker mindset as extreme reductionism: ignoring the surface abstractions and limitations to treat a system as a source of parts to manipulate into a different system, with different (and usually unintended) capabilities.",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Towards a Geometric Theory of Deep Learning - Govind Menon - YouTube",
    "url": "https://www.youtube.com/watch?v=44hfoihYfJ0",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "This lecture explores the mathematical underpinnings of deep learning, focusing on the deep linear network (DLN) model. The speaker presents sharp results on training dynamics, uncovering unexpected...",
    "full_snippet": "This lecture explores the mathematical underpinnings of deep learning, focusing on the deep linear network (DLN) model. The speaker presents sharp results on training dynamics, uncovering unexpected connections to minimal surfaces and geometric invariant theory. The analysis reveals a conceptual framework for understanding \"true\" deep learning. 1 of 78 lessons complete Follow along using the transcript. This lecture explores the mathematical underpinnings of deep learning, focusing on the deep linear network (DLN) model. The speaker presents sharp results on training dynamics, uncovering unexpected connections to minimal surfaces and geometric invariant theory. The analysis reveals a conceptual framework for understanding \"true\" deep learning. 1 of 78 lessons complete Follow along using the transcript.",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "Reinforcement Learning algorithms summarized",
    "url": "https://letters.lossfunk.com/p/reinforcement-learning-algorithms",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "The basic task of reinforcement learning is this: given a state we\u2019re in and probabilities of different actions we can take, how do we increase or decrease those probabilities so that we increase...",
    "full_snippet": "The basic task of reinforcement learning is this: given a state we\u2019re in and probabilities of different actions we can take, how do we increase or decrease those probabilities so that we increase average rewards in future. The fundamental tradeoff in this is bias, variance (as detailed below). Thanks for reading! Subscribe for free to receive new posts and support my work. The intuitive thing to do is to increase probability of actions that are \u201cbetter\u201d (in the sense of helping a higher rewarding trajectory in future) Notice that no matter which algorithm we end up using, we need to know how \u201crewarding\u201d an action is. This reward can be calculated in two ways: Montecarlo approach: Wait for full roll-outs and sum actual rewards we get in the trajectory This is high variance, low bias because you\u2019re working with actual rewards from environment (not estimates) but it has high variance because rewards for different trajectories varies by a lot Bootstrap an estimator of rewards: learn an est",
    "to_read": false,
    "signal_color": "#ef4444",
    "has_snippet": true
  },
  {
    "title": "The Love Song of J. Alfred Prufrock by T. S. Eliot | Poetry Magazine",
    "url": "https://www.poetryfoundation.org/poetrymagazine/poems/44212/the-love-song-of-j-alfred-prufrock",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "The yellow fog that rubs its back upon the window-panes, The yellow smoke that rubs its muzzle on the window-panes, Licked its tongue into the corners of the evening, Lingered upon the pools that...",
    "full_snippet": "The yellow fog that rubs its back upon the window-panes, The yellow smoke that rubs its muzzle on the window-panes, Licked its tongue into the corners of the evening, Lingered upon the pools that stand in drains, Let fall upon its back the soot that falls from chimneys, Slipped by the terrace, made\u2026",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "thalassophilia - by hannah - a lot about nothing",
    "url": "https://substack.com/inbox/post/181977520?r=294fgl",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "Lately I\u2019ve been restless as ever. I miss home and all its people and real seasons where all the leaves color the sky orange-red and dance swirling in the sweeping wind. I miss the vastness of the...",
    "full_snippet": "Lately I\u2019ve been restless as ever. I miss home and all its people and real seasons where all the leaves color the sky orange-red and dance swirling in the sweeping wind. I miss the vastness of the cornfields, so thick you could drown in it \u2014 a midwestern thalassophiliac. It\u2019s the kind of empty space that you can only fill with the thoughts of leaving (in all senses of the word). It\u2019s home, the same as it was 19 years ago and the same it will be in 19 more, just as I left it. How ironic is it that Indiana is a landlocked state? Recently, I came across the term oceanic feeling, coined by Romain Rolland in a letter to Freud. He used it to describe a sensation of \u2018eternity\u2019, an unbounded union with the universe. He attributed this incredible, metaphysical state to the source of all religious energy, the universal god. Freud, in his response, dismissed this as a deformity: a vestigial sensation left over from infancy, when the borders of the world haven\u2019t yet formed, and the edges of the eg",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "The Concept of the Ruliad\u2014Stephen Wolfram Writings",
    "url": "https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "I call it the ruliad. Think of it as the entangled limit of everything that is computationally possible: the result of following all possible computational rules in all possible ways. It\u2019s yet...",
    "full_snippet": "I call it the ruliad. Think of it as the entangled limit of everything that is computationally possible: the result of following all possible computational rules in all possible ways. It\u2019s yet another surprising construct that\u2019s arisen from our Physics Project. And it\u2019s one that I think has extremely deep implications\u2014both in science and beyond. In many ways, the ruliad is a strange and profoundly abstract thing. But it\u2019s something very universal\u2014a kind of ultimate limit of all abstraction and generalization. And it encapsulates not only all formal possibilities but also everything about our physical universe\u2014and everything we experience can be thought of as sampling that part of the ruliad that corresponds to our particular way of perceiving and interpreting the universe. We\u2019re going to be able to say many things about the ruliad without engaging in all its technical details. (And\u2014it should be said at the outset\u2014we\u2019re still only at the very beginning of nailing down those technical de",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Francesco Capuano",
    "url": "https://fracapuano.github.io/blog/optimizing-nns",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "TLDR: A technical blog to revisit the fundamentals of what, in the crudest sense, makes Deep Learning work. A SGD-to-Muon tour, derived from first principles in math and then implemented from scratch...",
    "full_snippet": "TLDR: A technical blog to revisit the fundamentals of what, in the crudest sense, makes Deep Learning work. A SGD-to-Muon tour, derived from first principles in math and then implemented from scratch in Jax. Take a NN parametrized with parameters \n\ud835\udf03\n\u03b8. During training, the parameters are updated using differential information relating the performance obtained to the weights used, i.e. using \n\u2207\n\ud835\udc3f\n(\n\ud835\udf03\n)\n=\n\u2211\n\ud835\udc56\n\u2208\n\ud835\udc37\n\u2207\n\u2113\n\ud835\udc56\n(\n\ud835\udf03\n)\n\u2207L(\u03b8)=\u2211\ni\u2208D\n\t\u200b\n\n\u2207\u2113\ni\n\t\u200b\n\n(\u03b8), so that weights are iteratively updated according to: where \n\ud835\udc53\nf is some function of the weights \n\ud835\udf03\n\ud835\udc61\n\u2212\n1\n\u03b8\nt\u22121\n\t\u200b\n\n and gradients \n\u2207\n\ud835\udc3f\n(\n\ud835\udf03\n\ud835\udc61\n\u2212\n1\n)\n\u2207L(\u03b8\nt\u22121\n\t\u200b\n\n). For both conceptual and computational reason, one typically does not use the exact gradient of the loss \n\u2207\n\ud835\udc3f\n(\n\ud835\udf03\n)\n\u2207L(\u03b8), and rather relies on \n1\n\u2223\n\ud835\udc35\n\u2223\n\u2211\n\ud835\udc56\n\u2208\n\ud835\udc35\n\u2207\n\u2113\n\ud835\udc56\n(\n\ud835\udf03\n)\n\u2223B\u2223\n1\n\t\u200b\n\n\u2211\ni\u2208B\n\t\u200b\n\n\u2207\u2113\ni\n\t\u200b\n\n(\u03b8), referred to as the stochastic gradient for the mini-batch \n\ud835\udc35\n\u2282\n\ud835\udc37\n:\n\ud835\udc35\n\u223c\n\ud835\udc37\nB\u2282D:B\u223cD. On a conceptual level, stochastic gradients suffer less from poor i",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  },
  {
    "title": "Just quit - Arjun Raj",
    "url": "https://arjunrajlab.substack.com/p/just-quit",
    "date": "December 31, 2025",
    "domain": "",
    "snippet": "We spend a lot of time as scientists thinking about how to choose a project\u2014and that is, of course, critically important to success. But\u2026 no matter how carefully you try to pick out the most...",
    "full_snippet": "We spend a lot of time as scientists thinking about how to choose a project\u2014and that is, of course, critically important to success. But\u2026 no matter how carefully you try to pick out the most groundbreaking, innovative project imaginable, the simple truth is that not every project is going to be awesome. Consequently, just as important as the skill of choosing a project is the skill of knowing when to quit a project. In my view, we should quit far more often than we do, for the simple reason that time is so very precious. Here is possibly the scariest diagram of all time: That is not a lot of weeks. Each scientific project can take up 2-4 ENTIRE COLUMNS. As mentioned, the success of a project is way more probabilistic than we care to admit. So you have to sample, and that means rejecting many samples. Do not let this precious time go to waste. Sometimes, you just have to quit. Why is it so hard to quit in science? Here are a few top reasons, all of which are based on fear: Fear of the u",
    "to_read": true,
    "signal_color": "#10b981",
    "has_snippet": true
  }
]